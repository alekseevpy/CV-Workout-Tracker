{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8wboqm8VPPs"
      },
      "source": [
        "### 1) Обучение модели LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvKwnX_Ed7bj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "import torchvision.transforms as T\n",
        "import torchvision.models.video as models\n",
        "from torchvision.models.video import R3D_18_Weights\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay,accuracy_score, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "# Config\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "NUM_FRAMES = 16\n",
        "IMG_SIZE = 112\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 20\n",
        "PATIENCE = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXfC3lQrb7dt"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2-3pLfNc6k0"
      },
      "outputs": [],
      "source": [
        "#!unzip -q /content/other.zip -d /content/data\n",
        "!unzip -q /content/Captures.zip -d /content/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wE3cAupaf8Zu",
        "outputId": "59bfd7dd-a8be-4a67-980c-835731f79c97"
      },
      "outputs": [],
      "source": [
        "#!unzip -q /content/drive/MyDrive/archive.zip -d /content/data\n",
        "\n",
        "# или\n",
        "!pip install gdown\n",
        "FILE_ID = '1TVhCj9cr5WMEmsyxGxUahuAcUXXXeQ7d'\n",
        "!gdown --id $FILE_ID\n",
        "!unzip -q /content/archive.zip -d /content/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzWlmwF-gFCu"
      },
      "outputs": [],
      "source": [
        "BASE_PATH = '/content/data'\n",
        "\n",
        "def get_all_videos(base_path):\n",
        "    videos = []\n",
        "    labels = []\n",
        "    for label in os.listdir(base_path):\n",
        "        folder = os.path.join(base_path, label)\n",
        "        if os.path.isdir(folder):\n",
        "            for file in os.listdir(folder):\n",
        "                if file.endswith(\".mp4\"):\n",
        "                    videos.append(os.path.join(folder, file))\n",
        "                    labels.append(label)\n",
        "    return videos, labels\n",
        "\n",
        "videos, labels = get_all_videos(BASE_PATH)\n",
        "le = LabelEncoder()\n",
        "encoded_labels = le.fit_transform(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gx7W2gxZaM6Q",
        "outputId": "bb3db52e-6d9a-4945-e140-f064ad29be9d"
      },
      "outputs": [],
      "source": [
        "for i, class_name in enumerate(le.classes_):\n",
        "    print(f\"Класс {i}: {class_name}\")\n",
        "\n",
        "import json\n",
        "class_mapping = {i: class_name for i, class_name in enumerate(le.classes_)}\n",
        "with open('class_mapping.json', 'w') as f:\n",
        "    json.dump(class_mapping, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhVesj_QgjX1",
        "outputId": "6630eaca-0206-4e51-87b2-b04198dd4114"
      },
      "outputs": [],
      "source": [
        "print(\"\\nСтруктура объединенного датасета:\")\n",
        "for class_name in sorted(os.listdir(BASE_PATH)):\n",
        "    class_path = os.path.join(BASE_PATH, class_name)\n",
        "    if os.path.isdir(class_path):\n",
        "        video_count = len([f for f in os.listdir(class_path)\n",
        "                          if f.lower().endswith(('.mp4', '.avi', '.mov', '.mkv'))])\n",
        "        print(f\"  {class_name}: {video_count} видео\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JbJYv9B1goeL",
        "outputId": "d8da8aba-7458-452b-9a50-081fa028e90e"
      },
      "outputs": [],
      "source": [
        "class_counts = Counter(labels)\n",
        "plt.figure(figsize=(15, 15))\n",
        "shown_labels = set()\n",
        "shown = 0\n",
        "for i, path in enumerate(videos):\n",
        "    label = labels[i]\n",
        "    if label in shown_labels:\n",
        "        continue\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    success, frame = cap.read()\n",
        "    cap.release()\n",
        "    if success:\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        plt.subplot(5, 5, shown + 1)\n",
        "        plt.imshow(frame)\n",
        "        plt.title(label)\n",
        "        plt.axis(\"off\")\n",
        "        shown_labels.add(label)\n",
        "        shown += 1\n",
        "    if shown >= len(class_counts):\n",
        "        break\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7KKE2B1Agr2p",
        "outputId": "9c3e8401-0224-40ea-9bf6-59cffa61f9a8"
      },
      "outputs": [],
      "source": [
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "eeTH5X4Cg1Ge",
        "outputId": "79eb57ba-3557-4f02-b6e4-4cb07c721aeb"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from ultralytics import YOLO\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Загружаем модель для позы\n",
        "pose_model = YOLO('yolo11s-pose.pt')\n",
        "\n",
        "def extract_pose_sequence(video_path, max_frames=60):\n",
        "    \"\"\"Извлекает последовательность поз из видео\"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    poses = []\n",
        "    frame_count = 0\n",
        "\n",
        "    while cap.isOpened() and frame_count < max_frames:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Детектируем позу\n",
        "        results = pose_model(frame, verbose=False)\n",
        "\n",
        "        if len(results[0].keypoints) > 0:\n",
        "            # Берем позу первого человека\n",
        "            keypoints = results[0].keypoints[0].data.cpu().numpy()[0]  # [17, 3]\n",
        "            poses.append(keypoints.flatten())\n",
        "        else:\n",
        "            # Если человека нет - заполняем нулями\n",
        "            poses.append(np.zeros(51))\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "    return np.array(poses)\n",
        "\n",
        "# Извлекаем последовательности для всех видео\n",
        "print(\"Извлекаем позы из видео...\")\n",
        "video_sequences = []\n",
        "\n",
        "for i, video_path in enumerate(videos):\n",
        "    if i % 10 == 0:\n",
        "        print(f\"Обработано {i}/{len(videos)} видео\")\n",
        "\n",
        "    sequence = extract_pose_sequence(video_path)\n",
        "    video_sequences.append(sequence)\n",
        "\n",
        "print(f\"Извлечено {len(video_sequences)} последовательностей\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJuKrOQ5g6TI",
        "outputId": "acb4146b-caa0-44fe-f1b5-a010a755fc29"
      },
      "outputs": [],
      "source": [
        "lengths = [len(seq) for seq in video_sequences]\n",
        "\n",
        "print(\"Статистика длин последовательностей:\")\n",
        "print(f\"Min: {np.min(lengths)}\")\n",
        "print(f\"Max: {np.max(lengths)}\")\n",
        "print(f\"Mean: {np.mean(lengths):.1f}\")\n",
        "print(f\"Median (50%): {np.median(lengths)}\")\n",
        "print(f\"75% перцентиль: {np.percentile(lengths, 75)}\")\n",
        "print(f\"90% перцентиль: {np.percentile(lengths, 90)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5unW0WPg8Ul"
      },
      "outputs": [],
      "source": [
        "class PoseDatasetWithAugmentation(Dataset):\n",
        "    def __init__(self, sequences, labels, seq_length=50, is_training=True, augment_prob=0.75):\n",
        "        self.sequences = sequences\n",
        "        self.labels = labels\n",
        "        self.seq_length = seq_length\n",
        "        self.is_training = is_training\n",
        "        self.augment_prob = augment_prob\n",
        "\n",
        "    def random_drop_frames(self, sequence, target_length):\n",
        "        \"\"\"\n",
        "        Удаляет случайные кадры из последовательности (имитация потерь)\n",
        "        sequence: [N, 51] - исходная последовательность\n",
        "        target_length: нужная длина\n",
        "        \"\"\"\n",
        "        if len(sequence) <= target_length:\n",
        "            return sequence\n",
        "\n",
        "        # Случайно выбираем какие кадры сохранить\n",
        "        indices_to_keep = np.random.choice(\n",
        "            len(sequence),\n",
        "            size=target_length,\n",
        "            replace=False\n",
        "        )\n",
        "        indices_to_keep.sort()\n",
        "\n",
        "        return sequence[indices_to_keep]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sequence = self.sequences[idx].copy()  # [N, 51]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Нормализуем позу\n",
        "        sequence = self.normalize_pose(sequence)\n",
        "\n",
        "        # Аугментации только в режиме обучения\n",
        "        if self.is_training and np.random.random() < self.augment_prob:\n",
        "            sequence = self.augment_sequence(sequence)\n",
        "\n",
        "        # Приводим к фиксированной длине\n",
        "        processed_seq = self.pad_or_truncate(sequence)\n",
        "\n",
        "        return torch.FloatTensor(processed_seq), torch.tensor(label)\n",
        "\n",
        "    def normalize_pose(self, sequence):\n",
        "        \"\"\"Нормализует координаты относительно таза\"\"\"\n",
        "        if len(sequence) == 0:\n",
        "            return sequence\n",
        "\n",
        "        seq_reshaped = sequence.reshape(-1, 17, 3)\n",
        "\n",
        "        for i in range(len(seq_reshaped)):\n",
        "            frame = seq_reshaped[i]\n",
        "\n",
        "            # Находим центр таза (точки 11, 12)\n",
        "            if np.all(frame[11, 2] > 0.1) and np.all(frame[12, 2] > 0.1):\n",
        "                pelvis_center = (frame[11, :2] + frame[12, :2]) / 2\n",
        "            else:\n",
        "                # Используем среднее всех видимых точек\n",
        "                visible_points = frame[frame[:, 2] > 0.1]\n",
        "                if len(visible_points) > 0:\n",
        "                    pelvis_center = np.mean(visible_points[:, :2], axis=0)\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "            # Центрируем все точки\n",
        "            seq_reshaped[i, :, :2] = seq_reshaped[i, :, :2] - pelvis_center\n",
        "\n",
        "        return seq_reshaped.reshape(-1, 51)\n",
        "\n",
        "    def augment_sequence(self, sequence):\n",
        "        \"\"\"Применяет аугментации к последовательности поз\"\"\"\n",
        "        if len(sequence) == 0:\n",
        "            return sequence\n",
        "\n",
        "        seq_reshaped = sequence.reshape(-1, 17, 3)\n",
        "\n",
        "        # 1. Зеркалирование (горизонтальное отражение)\n",
        "        if np.random.random() < 0.5:\n",
        "            seq_reshaped = self.mirror_sequence(seq_reshaped)\n",
        "\n",
        "        # 2. Небольшой поворот (2-5 градусов)\n",
        "        if np.random.random() < 0.5:\n",
        "            angle = np.random.uniform(-10, 10)  # градусы\n",
        "            seq_reshaped = self.rotate_sequence(seq_reshaped, angle)\n",
        "\n",
        "        # 3. Случайный сдвиг точек\n",
        "        if np.random.random() < 0.5:\n",
        "            noise_std = np.random.uniform(0.05, 0.09)\n",
        "            seq_reshaped = self.add_jitter(seq_reshaped, noise_std)\n",
        "\n",
        "        # 4. Случайное масштабирование\n",
        "        if np.random.random() < 0.25:\n",
        "            scale = np.random.uniform(0.9, 1.1)\n",
        "            seq_reshaped[:, :, :2] = seq_reshaped[:, :, :2] * scale\n",
        "\n",
        "        return seq_reshaped.reshape(-1, 51)\n",
        "\n",
        "    def mirror_sequence(self, sequence):\n",
        "        \"\"\"Зеркалирует позу (горизонтальное отражение)\"\"\"\n",
        "        mirrored = sequence.copy()\n",
        "\n",
        "        # Инвертируем x координаты\n",
        "        mirrored[:, :, 0] = -mirrored[:, :, 0]\n",
        "\n",
        "        # Меняем местами левые и правые суставы\n",
        "        left_right_pairs = [(1, 2), (3, 4), (5, 6), (7, 8), (9, 10), (11, 12), (13, 14), (15, 16)]\n",
        "\n",
        "        for left_idx, right_idx in left_right_pairs:\n",
        "            temp = mirrored[:, left_idx, :].copy()\n",
        "            mirrored[:, left_idx, :] = mirrored[:, right_idx, :]\n",
        "            mirrored[:, right_idx, :] = temp\n",
        "\n",
        "        return mirrored\n",
        "\n",
        "    def rotate_sequence(self, sequence, angle_degrees):\n",
        "        \"\"\"Поворачивает позу на небольшой угол\"\"\"\n",
        "        angle_rad = np.radians(angle_degrees)\n",
        "        rotation_matrix = np.array([\n",
        "            [np.cos(angle_rad), -np.sin(angle_rad)],\n",
        "            [np.sin(angle_rad), np.cos(angle_rad)]\n",
        "        ])\n",
        "\n",
        "        rotated = sequence.copy()\n",
        "        rotated[:, :, :2] = np.dot(rotated[:, :, :2], rotation_matrix.T)\n",
        "\n",
        "        return rotated\n",
        "\n",
        "    def add_jitter(self, sequence, noise_std):\n",
        "        \"\"\"Добавляет случайный шум к координатам\"\"\"\n",
        "        jittered = sequence.copy()\n",
        "        noise = np.random.normal(0, noise_std, jittered[:, :, :2].shape)\n",
        "        jittered[:, :, :2] += noise\n",
        "        return jittered\n",
        "\n",
        "    def pad_or_truncate(self, sequence):\n",
        "        \"\"\"Новая версия - удаляем случайные кадры вместо обрезки концов\"\"\"\n",
        "        if len(sequence) > self.seq_length:\n",
        "            # Вместо обрезки конца - удаляем случайные кадры\n",
        "            sequence = self.random_drop_frames(sequence, self.seq_length)\n",
        "        else:\n",
        "            # Дополняем нулями как раньше\n",
        "            padded = np.zeros((self.seq_length, 51))\n",
        "            padded[:len(sequence)] = sequence\n",
        "            sequence = padded\n",
        "\n",
        "        return sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2PVtGtthDT4",
        "outputId": "a25a157c-1a8f-4930-d54d-2424ddc4fe75"
      },
      "outputs": [],
      "source": [
        "def prepare_lstm_data(video_sequences, labels, test_size=0.20):\n",
        "    \"\"\"Подготавливает данные для LSTM\"\"\"\n",
        "    # Разделяем на train/test\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        video_sequences, labels,\n",
        "        test_size=test_size, random_state=42, stratify=labels\n",
        "    )\n",
        "\n",
        "    # Создаем датасеты\n",
        "    train_dataset = PoseDatasetWithAugmentation(X_train, y_train, seq_length=50, is_training=True)\n",
        "    test_dataset = PoseDatasetWithAugmentation(X_test, y_test, seq_length=50, is_training=False)\n",
        "\n",
        "    # Создаем DataLoader'ы\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "    return train_loader, test_loader, len(np.unique(labels))\n",
        "\n",
        "# Подготавливаем данные\n",
        "train_loader, test_loader, num_classes = prepare_lstm_data(video_sequences, encoded_labels)\n",
        "print(f\"Количество классов: {num_classes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtdFhUuihHp1"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "class EnhancedPoseLSTM(nn.Module):\n",
        "    def __init__(self, input_size=51, hidden_size=256, num_layers=3, num_classes=23, dropout=0.4):\n",
        "        super(EnhancedPoseLSTM, self).__init__()\n",
        "\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Более глубокая LSTM архитектура\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        # Улучшенный attention механизм\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(hidden_size * 2, 128),\n",
        "            nn.Tanh(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(128, 32),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "\n",
        "        # Более глубокая классифицирующая сеть\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size * 2, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(512),\n",
        "\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(256),\n",
        "\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(128),\n",
        "\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # LSTM forward\n",
        "        lstm_out, (hidden, cell) = self.lstm(x)  # [batch_size, seq_len, hidden_size*2]\n",
        "\n",
        "        # Multi-head attention (улучшенная версия)\n",
        "        attention_weights = torch.softmax(\n",
        "            self.attention(lstm_out).squeeze(-1), dim=1\n",
        "        )\n",
        "\n",
        "        # Взвешенная сумма с attention\n",
        "        context_vector = torch.bmm(\n",
        "            attention_weights.unsqueeze(1), lstm_out\n",
        "        ).squeeze(1)\n",
        "\n",
        "        # Классификация\n",
        "        output = self.classifier(context_vector)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0iIItC1hIFy"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = EnhancedPoseLSTM(num_classes=num_classes, dropout=0.4).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gk_lX3WShJ4S"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def train_model_with_early_stopping(model, train_loader, test_loader, num_epochs=100, patience=15):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=7, factor=0.5)\n",
        "\n",
        "    best_accuracy = 0\n",
        "    patience_counter = 0\n",
        "    train_losses = []\n",
        "    test_accuracies = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Обучение\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping для стабильности\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Валидация\n",
        "        model.eval()\n",
        "        test_correct = 0\n",
        "        test_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data, targets in test_loader:\n",
        "                data, targets = data.to(device), targets.to(device)\n",
        "                outputs = model(data)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                test_total += targets.size(0)\n",
        "                test_correct += (predicted == targets).sum().item()\n",
        "\n",
        "        test_accuracy = 100 * test_correct / test_total\n",
        "        test_accuracies.append(test_accuracy)\n",
        "        train_losses.append(running_loss / len(train_loader))\n",
        "\n",
        "        scheduler.step(test_accuracy)\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
        "              f'Loss: {running_loss/len(train_loader):.4f}, '\n",
        "              f'Test Acc: {test_accuracy:.2f}%')\n",
        "\n",
        "        # Early stopping\n",
        "        if test_accuracy > best_accuracy:\n",
        "            best_accuracy = test_accuracy\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), 'best_pose_lstm.pth')\n",
        "            print(f\"Новый лучший результат! Сохранена модель с точностью {best_accuracy:.2f}%\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping на эпохе {epoch+1}\")\n",
        "            break\n",
        "\n",
        "    # Загружаем лучшую модель\n",
        "    model.load_state_dict(torch.load('best_pose_lstm.pth'))\n",
        "    return train_losses, test_accuracies, best_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Z30ZosIkbnf",
        "outputId": "469fbdb1-922b-49df-845a-eb9c7471f7c3"
      },
      "outputs": [],
      "source": [
        "# Запускаем обучение\n",
        "print(\"Начинаем обучение LSTM с аугментациями...\")\n",
        "train_losses, test_accuracies, best_acc = train_model_with_early_stopping(\n",
        "    model, train_loader, test_loader, num_epochs=150, patience=20)\n",
        "\n",
        "print(f\"Лучшая точность: {best_acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23zJq9e9lCJb"
      },
      "source": [
        "### Взаимодействие с готовой моделью (ПЕРЕВЕДЕНО В ПАЙПЛАЙН, ВЫПОЛНЯТЬ НЕ НАДО)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tqk4hpdFWP2g",
        "outputId": "95b95384-1911-4038-9ce9-48d7ac4655fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ],
      "source": [
        "#ФУНКЦИЯ ДЕТЕКТИТ И ТРЕКАЕТ ЛЮДЕЙ\n",
        "import cv2\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "from collections import defaultdict\n",
        "def process_video_for_lstm(video_path, min_visibility_seconds=5, min_detection_percent=40):\n",
        "    \"\"\"\n",
        "    Трекинг людей на видео с фильтрацией\n",
        "\n",
        "    Возвращает: {track_id: {\"keypoints_original\": [...], ...}}\n",
        "    \"\"\"\n",
        "    # Загрузка модели\n",
        "    model = YOLO('yolo11n-pose.pt')\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    tracker_config = {\n",
        "      'tracker_type': 'bytetrack',\n",
        "      'track_high_thresh': 0.6,\n",
        "      'track_low_thresh': 0.3,\n",
        "      'new_track_thresh': 0.7,\n",
        "      'track_buffer': 10,\n",
        "      'match_thresh': 0.70,\n",
        "      'fuse_score': True}\n",
        "\n",
        "    import tempfile\n",
        "    import yaml\n",
        "    with tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False) as f:\n",
        "      yaml.dump(tracker_config, f)\n",
        "      tracker_file = f.name\n",
        "\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    if fps <= 0:\n",
        "        fps = 30\n",
        "\n",
        "    # Хранилище данных\n",
        "    tracks = defaultdict(lambda: {\n",
        "        \"keypoints_original\": [],  # ТОЛЬКО оригинальные координаты\n",
        "        \"timestamps\": [],\n",
        "        \"frame_indices\": [],\n",
        "        \"confidences\": [],\n",
        "        \"bboxes\": []\n",
        "    })\n",
        "\n",
        "    # Основной цикл трекинга\n",
        "    frame_idx = 0\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Детекция с трекингом\n",
        "        results = model.track(frame, persist=True, verbose=False, conf=0.5, iou=0.4, tracker=tracker_file)\n",
        "\n",
        "        # Обработка детекций\n",
        "        if (results[0].boxes is not None and\n",
        "            results[0].boxes.id is not None and\n",
        "            results[0].keypoints is not None):\n",
        "\n",
        "            track_ids = results[0].boxes.id.cpu().numpy().astype(int)\n",
        "            keypoints_list = results[0].keypoints.data.cpu().numpy()\n",
        "\n",
        "            for i, track_id in enumerate(track_ids):\n",
        "                kps = keypoints_list[i]\n",
        "                if len(kps.shape) == 3:\n",
        "                    kps = kps[0]  # [17, 3]\n",
        "\n",
        "                # Сохраняем ТОЛЬКО оригинальные координаты\n",
        "                tracks[track_id][\"keypoints_original\"].append(kps.flatten())\n",
        "                tracks[track_id][\"timestamps\"].append(frame_idx / fps)\n",
        "                tracks[track_id][\"frame_indices\"].append(frame_idx)\n",
        "\n",
        "        frame_idx += 1\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    # ФИЛЬТРАЦИЯ\n",
        "    filtered_tracks = {}\n",
        "\n",
        "    for track_id, data in tracks.items():\n",
        "        if len(data[\"timestamps\"]) == 0:\n",
        "            continue\n",
        "\n",
        "        # Вычисляем метрики\n",
        "        start_time = data[\"timestamps\"][0]\n",
        "        end_time = data[\"timestamps\"][-1]\n",
        "        duration = end_time - start_time\n",
        "\n",
        "        # Вычисляем процент кадров с детекцией\n",
        "        detection_windows = []\n",
        "        window_size = 1.0  # 1 секунда\n",
        "\n",
        "        current_window_start = start_time\n",
        "        while current_window_start < end_time:\n",
        "            window_end = current_window_start + window_size\n",
        "\n",
        "            # Кадры в этом окне\n",
        "            frames_in_window = [\n",
        "                i for i, t in enumerate(data[\"timestamps\"])\n",
        "                if current_window_start <= t < window_end\n",
        "            ]\n",
        "\n",
        "            if len(frames_in_window) > 0:\n",
        "                detection_windows.append(len(frames_in_window))\n",
        "\n",
        "            current_window_start = window_end\n",
        "\n",
        "        # Если были окна с детекцией\n",
        "        if detection_windows:\n",
        "            avg_detection_in_window = np.mean(detection_windows)\n",
        "            expected_frames_per_window = window_size * fps\n",
        "            detection_percentage = (avg_detection_in_window / expected_frames_per_window) * 100\n",
        "        else:\n",
        "            detection_percentage = 0\n",
        "\n",
        "        # Критерии фильтрации\n",
        "        condition1 = (duration >= min_visibility_seconds and\n",
        "                     detection_percentage >= min_detection_percent)\n",
        "        condition2 = (detection_percentage >= 80)\n",
        "\n",
        "        if condition1 or condition2:\n",
        "            # Конвертируем в numpy\n",
        "            filtered_tracks[track_id] = {\n",
        "                \"keypoints_original\": np.array(data[\"keypoints_original\"]),  # Только оригинальные\n",
        "                \"timestamps\": np.array(data[\"timestamps\"]),\n",
        "                \"frame_indices\": np.array(data[\"frame_indices\"]),\n",
        "                \"duration\": duration,\n",
        "                \"detection_percentage\": detection_percentage\n",
        "            }\n",
        "    return filtered_tracks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPrjslAr188y"
      },
      "outputs": [],
      "source": [
        "class PosePreprocessor:\n",
        "    \"\"\"Класс для предобработки последовательностей ключевых точек\"\"\"\n",
        "    def __init__(self, seq_length=50, training=False):\n",
        "        self.seq_length = seq_length\n",
        "        self.training = training\n",
        "\n",
        "    def random_drop_frames(self, sequence, target_length):\n",
        "        \"\"\"\n",
        "        Удаляет случайные кадры из последовательности (имитация потерь)\n",
        "        sequence: [N, 51] - исходная последовательность\n",
        "        target_length: нужная длина\n",
        "        \"\"\"\n",
        "        if len(sequence) <= target_length:\n",
        "            return sequence\n",
        "\n",
        "        # Случайно выбираем какие кадры сохранить\n",
        "        indices_to_keep = np.random.choice(\n",
        "            len(sequence),\n",
        "            size=target_length,\n",
        "            replace=False\n",
        "        )\n",
        "        indices_to_keep.sort()\n",
        "\n",
        "        return sequence[indices_to_keep]\n",
        "\n",
        "    def normalize_pose(self, sequence):\n",
        "        \"\"\"Нормализует координаты относительно таза\"\"\"\n",
        "        if len(sequence) == 0:\n",
        "            return sequence\n",
        "\n",
        "        seq_reshaped = sequence.reshape(-1, 17, 3)\n",
        "\n",
        "        for i in range(len(seq_reshaped)):\n",
        "            frame = seq_reshaped[i]\n",
        "\n",
        "            # Находим центр таза (точки 11, 12)\n",
        "            if np.all(frame[11, 2] > 0.1) and np.all(frame[12, 2] > 0.1):\n",
        "                #print('awesome norm')\n",
        "                pelvis_center = (frame[11, :2] + frame[12, :2]) / 2\n",
        "            else:\n",
        "                # Используем среднее всех видимых точек\n",
        "                visible_points = frame[frame[:, 2] > 0.1]\n",
        "                if len(visible_points) > 0:\n",
        "                    #print('meh norm')\n",
        "                    pelvis_center = np.mean(visible_points[:, :2], axis=0)\n",
        "                else:\n",
        "                    #print('no norm')\n",
        "                    continue\n",
        "\n",
        "            # Центрируем все точки\n",
        "            seq_reshaped[i, :, :2] = seq_reshaped[i, :, :2] - pelvis_center\n",
        "\n",
        "        return seq_reshaped.reshape(-1, 51)\n",
        "\n",
        "    def pad_or_truncate(self, sequence):\n",
        "        \"\"\"Приводит последовательность к фиксированной длине self.seq_length\"\"\"\n",
        "        if len(sequence) > self.seq_length:\n",
        "            # Вместо обрезки конца - удаляем случайные кадры\n",
        "            sequence = self.random_drop_frames(sequence, self.seq_length)\n",
        "        else:\n",
        "            # Дополняем нулями\n",
        "            padded = np.zeros((self.seq_length, 51))\n",
        "            padded[:len(sequence)] = sequence\n",
        "            sequence = padded\n",
        "\n",
        "        return sequence\n",
        "\n",
        "    def preprocess(self, keypoints_sequence):\n",
        "        \"\"\"\n",
        "        Полный пайплайн предобработки для одного сегмента\n",
        "        keypoints_sequence: [n_frames, 51]\n",
        "        возвращает: [seq_length, 51]\n",
        "        \"\"\"\n",
        "        keypoints_sequence_copy = keypoints_sequence.copy()\n",
        "        # 1. Нормализация\n",
        "        normalized = self.normalize_pose(keypoints_sequence_copy)\n",
        "\n",
        "        # 2. Приведение к фиксированной длине (60 кадров)\n",
        "        processed = self.pad_or_truncate(normalized)\n",
        "\n",
        "        return processed\n",
        "\n",
        "\n",
        "def load_lstm_model(model_path, class_names, device='cpu'):\n",
        "    \"\"\"\n",
        "    Загружает LSTM модель с весами\n",
        "    \"\"\"\n",
        "    print(f\"Загрузка модели из {model_path}...\")\n",
        "\n",
        "    model = EnhancedPoseLSTM(\n",
        "        input_size=51,\n",
        "        hidden_size=256,\n",
        "        num_layers=3,\n",
        "        num_classes=23,\n",
        "        dropout=0.4\n",
        "    )\n",
        "\n",
        "    # Загружаем веса\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    print(f\"Модель загружена на {device}\")\n",
        "    return model\n",
        "\n",
        "def classify_person_sequence(segment, lstm_model, preprocessor, class_names, device='cpu'):\n",
        "    \"\"\"\n",
        "    Реальная классификация с вашей LSTM моделью\n",
        "    \"\"\"\n",
        "    keypoints = segment[\"keypoints\"]  # [n_frames, 51] - ОРИГИНАЛЬНЫЕ координаты\n",
        "\n",
        "    if len(keypoints) == 0:\n",
        "        return {\n",
        "            \"predicted_class\": \"no_exercise\",\n",
        "            \"confidence\": 0.0,\n",
        "            \"class_idx\": -1\n",
        "        }\n",
        "\n",
        "    # ВАЖНО: preprocessor.preprocess() нормализует координаты внутри себя\n",
        "    # Он вызывает normalize_pose() → вычитает pelvis_center\n",
        "    processed = preprocessor.preprocess(keypoints)  # [50, 51] - НОРМАЛИЗОВАННЫЕ координаты\n",
        "\n",
        "    debug = False\n",
        "    if debug:\n",
        "        print(f\"До нормализации: X [{np.min(keypoints[:, 0::3]):.1f}, {np.max(keypoints[:, 0::3]):.1f}], Y [{np.min(keypoints[:, 1::3]):.1f}, {np.max(keypoints[:, 1::3]):.1f}]\")\n",
        "        print(f\"После нормализации: X [{np.min(processed[:, 0::3]):.1f}, {np.max(processed[:, 0::3]):.1f}], Y [{np.min(processed[:, 1::3]):.1f}, {np.max(processed[:, 1::3]):.1f}]\")\n",
        "\n",
        "    # Подготовка тензора\n",
        "    input_tensor = torch.tensor(processed, dtype=torch.float32)\n",
        "    input_tensor = input_tensor.unsqueeze(0).to(device)  # [1, 50, 51]\n",
        "\n",
        "    # Предсказание\n",
        "    with torch.no_grad():\n",
        "        outputs = lstm_model(input_tensor)\n",
        "        probabilities = torch.softmax(outputs, dim=1)\n",
        "        confidence, predicted_idx = torch.max(probabilities, dim=1)\n",
        "\n",
        "    return {\n",
        "        \"predicted_class\": class_names[predicted_idx.item()],\n",
        "        \"confidence\": confidence.item(),\n",
        "        \"class_idx\": predicted_idx.item()\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NULHx-xblG0q"
      },
      "outputs": [],
      "source": [
        "# ОСНОВАНЯ ФУНКЦИЯ, КОТОРАЯ ИДЕТ ПО ВИДЕО С НУЖНЫМ ОКНОМ\n",
        "def full_process(video_path, model_path, min_visibility_sec=5, min_detection_percent=40):\n",
        "    \"\"\"\n",
        "    Исправленная версия full_process\n",
        "    \"\"\"\n",
        "    # Конфигурация\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    # Загружаем LSTM модель\n",
        "    lstm_model = load_lstm_model(model_path, [], device)\n",
        "    preprocessor = PosePreprocessor(seq_length=50, training=False)\n",
        "\n",
        "    # 1. Трекинг - получаем только оригинальные координаты\n",
        "    print(f\"Трекинг видео: {video_path}\")\n",
        "    tracks = process_video_for_lstm(\n",
        "        video_path=video_path,\n",
        "        min_visibility_seconds=min_visibility_sec,\n",
        "        min_detection_percent=min_detection_percent\n",
        "    )\n",
        "    clean_tracks = tracks\n",
        "    if not tracks:\n",
        "        print(\"Нет людей для анализа\")\n",
        "        return {}, {}\n",
        "    # 2. Для каждого человека разбиваем на 2-секундные сегменты\n",
        "    predictions = {}\n",
        "\n",
        "    for track_id, track_data in tracks.items():\n",
        "        print(f\"\\nОбработка человека ID {track_id}\")\n",
        "        # Используем ОРИГИНАЛЬНЫЕ координаты\n",
        "        keypoints_original = track_data[\"keypoints_original\"]\n",
        "        timestamps = track_data[\"timestamps\"]\n",
        "\n",
        "        if len(timestamps) < 2:\n",
        "            continue\n",
        "\n",
        "        # Получаем FPS видео\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        if fps <= 0:\n",
        "            fps = 30\n",
        "        cap.release()\n",
        "\n",
        "        window_frames = 50#int(2 * fps)  # 2 секунды\n",
        "        step_frames = 50#int(1 * fps)    # 1 секунда перекрытия\n",
        "\n",
        "        segments = []\n",
        "\n",
        "        for start in range(0, len(keypoints_original) - window_frames + 1, step_frames):\n",
        "            end = start + window_frames\n",
        "\n",
        "            segment = {\n",
        "                \"track_id\": track_id,\n",
        "                \"start_time\": float(timestamps[start]),\n",
        "                \"end_time\": float(timestamps[end-1]),\n",
        "                \"keypoints\": keypoints_original[start:end],  # Оригинальные координаты\n",
        "                \"segment_idx\": len(segments)\n",
        "            }\n",
        "            segments.append(segment)\n",
        "\n",
        "        print(f\"  Сегментов: {len(segments)}\")\n",
        "\n",
        "        # 3. Классификация каждого сегмента|\n",
        "        segment_predictions = []\n",
        "        import json\n",
        "        with open('class_mapping.json', 'r') as f:\n",
        "          class_mapping = json.load(f)\n",
        "        class_names = [class_mapping[str(i)] for i in range(len(class_mapping))]\n",
        "        for segment in segments:\n",
        "            prediction = classify_person_sequence(\n",
        "                segment=segment,\n",
        "                lstm_model=lstm_model,\n",
        "                preprocessor=preprocessor,\n",
        "                class_names=class_names,\n",
        "                device=device\n",
        "            )\n",
        "\n",
        "            result = {\n",
        "                \"track_id\": track_id,\n",
        "                \"segment_idx\": segment[\"segment_idx\"],\n",
        "                \"start_time\": segment[\"start_time\"],\n",
        "                \"end_time\": segment[\"end_time\"],\n",
        "                \"predicted_class\": prediction[\"predicted_class\"],\n",
        "                \"confidence\": prediction[\"confidence\"],\n",
        "                \"class_idx\": prediction[\"class_idx\"]\n",
        "            }\n",
        "            segment_predictions.append(result)\n",
        "\n",
        "        predictions[track_id] = segment_predictions\n",
        "    return clean_tracks, predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "sBPzKeEv2RYX",
        "outputId": "2346bbf9-717a-4a67-9d45-4498a156281c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Загрузка модели из /content/best_pose_lstm_new.pth...\n",
            "Модель загружена на cuda\n",
            "Трекинг видео: /content/test_final.mp4\n",
            "\n",
            "Обработка человека ID 1\n",
            "  Сегментов: 20\n",
            "\n",
            "Обработка человека ID 5\n",
            "  Сегментов: 6\n",
            "\n",
            "Обработка человека ID 7\n",
            "  Сегментов: 0\n",
            "\n",
            "Обработка человека ID 9\n",
            "  Сегментов: 1\n",
            "\n",
            "Обработка человека ID 20\n",
            "  Сегментов: 0\n",
            "\n",
            "Обработка человека ID 21\n",
            "  Сегментов: 1\n"
          ]
        }
      ],
      "source": [
        "tracks, predictions = full_process(\n",
        "      video_path=\"/content/test_final.mp4\",\n",
        "      model_path=\"/content/best_pose_lstm_new.pth\",\n",
        "      min_visibility_sec=2,\n",
        "      min_detection_percent=60\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erCyaamGjnAF"
      },
      "source": [
        "### Расчет статистик (ВАЖНАЯ ШТУКА)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WKvQeqojrN8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "import os\n",
        "\n",
        "def calculate_track_statistics(tracks, predictions):\n",
        "    \"\"\"\n",
        "    Рассчитывает статистику по каждому треку с фильтрацией по количеству детекций\n",
        "\n",
        "    Параметры:\n",
        "    -----------\n",
        "    tracks : dict\n",
        "        Словарь с треками от функции process_video_for_lstm\n",
        "    predictions : dict\n",
        "        Словарь с предсказаниями от функции full_process\n",
        "\n",
        "    Возвращает:\n",
        "    -----------\n",
        "    dict : отфильтрованная статистика по каждому треку\n",
        "    DataFrame : таблица со статистикой\n",
        "    \"\"\"\n",
        "\n",
        "    statistics = {}\n",
        "    filtered_tracks = {}\n",
        "\n",
        "    # Фильтрация треков по количеству детекций (сегментов)\n",
        "    print(f\"🔍 Фильтрация треков: минимум 3 детекции (сегмента)\")\n",
        "    original_count = len(tracks)\n",
        "\n",
        "    for track_id in tracks.keys():\n",
        "        # Количество сегментов для этого трека\n",
        "        track_segments = predictions.get(track_id, [])\n",
        "        segments_count = len(track_segments)\n",
        "\n",
        "        if segments_count > 2:  # Больше 2 детекций\n",
        "            filtered_tracks[track_id] = tracks[track_id]\n",
        "            print(f\"  Сохранен трек {track_id}: {segments_count} сегментов\")\n",
        "        else:\n",
        "            print(f\"  Пропущен трек {track_id}: только {segments_count} сегментов\")\n",
        "\n",
        "    print(f\"📊 После фильтрации треков: {len(filtered_tracks)} из {original_count} треков\")\n",
        "\n",
        "    # Расчет статистики по отфильтрованным трекам\n",
        "    for track_id, track_data in filtered_tracks.items():\n",
        "        # 1. Суммарное время видимости (из tracks)\n",
        "        total_time_visible = track_data.get(\"duration\", 0)\n",
        "\n",
        "        # Получаем timestamps для расчетов\n",
        "        timestamps = track_data.get(\"timestamps\", None)\n",
        "\n",
        "        if timestamps is not None and len(timestamps) > 0:\n",
        "            start_time = float(timestamps[0])\n",
        "            end_time = float(timestamps[-1])\n",
        "        else:\n",
        "            start_time = 0\n",
        "            end_time = 0\n",
        "\n",
        "        # 2. Собираем информацию об упражнениях\n",
        "        exercises_info = defaultdict(lambda: {\n",
        "            \"total_time\": 0,\n",
        "            \"segments\": 0,\n",
        "            \"segment_details\": []\n",
        "        })\n",
        "\n",
        "        if track_id in predictions and predictions[track_id]:\n",
        "            for segment in predictions[track_id]:\n",
        "                exercise = segment[\"predicted_class\"]\n",
        "                segment_duration = segment[\"end_time\"] - segment[\"start_time\"]\n",
        "\n",
        "                # Пропускаем \"no_exercise\"\n",
        "                if exercise == \"no_exercise\":\n",
        "                    continue\n",
        "\n",
        "                # Сохраняем детали сегмента\n",
        "                segment_info = {\n",
        "                    \"start_time\": segment[\"start_time\"],\n",
        "                    \"end_time\": segment[\"end_time\"],\n",
        "                    \"duration\": segment_duration,\n",
        "                    \"confidence\": segment.get(\"confidence\", 0),\n",
        "                    \"segment_idx\": segment.get(\"segment_idx\", 0)\n",
        "                }\n",
        "                exercises_info[exercise][\"segment_details\"].append(segment_info)\n",
        "\n",
        "        # 3. Фильтрация упражнений: минимум 3 детекции для учета\n",
        "        filtered_exercises_info = {}\n",
        "        filtered_out_exercises = []\n",
        "\n",
        "        for exercise, info in exercises_info.items():\n",
        "            segments_count = len(info[\"segment_details\"])\n",
        "\n",
        "            if segments_count > 2:  # Больше 2 детекций\n",
        "                # Суммируем общее время упражнения\n",
        "                total_time = sum(seg[\"duration\"] for seg in info[\"segment_details\"])\n",
        "                filtered_exercises_info[exercise] = {\n",
        "                    \"total_time\": total_time,\n",
        "                    \"segments\": segments_count\n",
        "                }\n",
        "                print(f\"  Трек {track_id}: упражнение '{exercise}' - {segments_count} сегментов\")\n",
        "            else:\n",
        "                filtered_out_exercises.append(exercise)\n",
        "\n",
        "        if filtered_out_exercises:\n",
        "            print(f\"  Трек {track_id}: пропущены упражнения {filtered_out_exercises} (≤2 сегментов)\")\n",
        "\n",
        "        # 4. Количество разнообразных упражнений (исключая \"other\")\n",
        "        unique_exercises = set()\n",
        "        for exercise in filtered_exercises_info.keys():\n",
        "            if exercise != \"other\" and exercise != \"no_exercise\":\n",
        "                unique_exercises.add(exercise)\n",
        "        num_unique_exercises = len(unique_exercises)\n",
        "\n",
        "        # 5. Время выполнения каждого упражнения относительно суммарного времени\n",
        "        exercise_percentages = {}\n",
        "        for exercise, info in filtered_exercises_info.items():\n",
        "            if exercise != \"other\" and exercise != \"no_exercise\":\n",
        "                percentage = (info[\"total_time\"] / total_time_visible * 100) if total_time_visible > 0 else 0\n",
        "                exercise_percentages[exercise] = {\n",
        "                    \"time_seconds\": round(info[\"total_time\"], 2),\n",
        "                    \"percentage\": round(percentage, 1),\n",
        "                    \"segments\": info[\"segments\"]\n",
        "                }\n",
        "\n",
        "        # 6. Время без упражнений \"other\" и \"no_exercise\"\n",
        "        total_time_without_other = total_time_visible\n",
        "        if \"other\" in filtered_exercises_info:\n",
        "            total_time_without_other -= filtered_exercises_info[\"other\"][\"total_time\"]\n",
        "\n",
        "        # Собираем всю статистику по треку\n",
        "        statistics[track_id] = {\n",
        "            \"track_id\": track_id,\n",
        "            \"total_time_visible_seconds\": round(total_time_visible, 2),\n",
        "            \"total_time_without_other_seconds\": round(total_time_without_other, 2),\n",
        "            \"num_unique_exercises\": num_unique_exercises,\n",
        "            \"exercises\": exercise_percentages,\n",
        "            \"total_segments\": len(predictions.get(track_id, [])),\n",
        "            \"detection_percentage\": track_data.get(\"detection_percentage\", 0),\n",
        "            \"frames_count\": len(track_data.get(\"keypoints_original\", [])),\n",
        "            \"start_time\": start_time,\n",
        "            \"end_time\": end_time,\n",
        "            \"filtered_out_exercises\": len(filtered_out_exercises),\n",
        "            \"filtered_out_exercises_list\": filtered_out_exercises\n",
        "        }\n",
        "\n",
        "    # Преобразуем в DataFrame для удобства\n",
        "    df_rows = []\n",
        "\n",
        "    for track_id, stats in statistics.items():\n",
        "        # Базовая строка\n",
        "        row = {\n",
        "            \"track_id\": track_id,\n",
        "            \"total_time_visible_seconds\": stats[\"total_time_visible_seconds\"],\n",
        "            \"total_time_without_other_seconds\": stats[\"total_time_without_other_seconds\"],\n",
        "            \"num_unique_exercises\": stats[\"num_unique_exercises\"],\n",
        "            \"total_segments\": stats[\"total_segments\"],\n",
        "            \"detection_percentage\": stats[\"detection_percentage\"],\n",
        "            \"frames_count\": stats[\"frames_count\"],\n",
        "            \"start_time\": stats[\"start_time\"],\n",
        "            \"end_time\": stats[\"end_time\"],\n",
        "            \"time_other_seconds\": stats[\"total_time_visible_seconds\"] - stats[\"total_time_without_other_seconds\"],\n",
        "            \"other_percentage\": round((stats[\"total_time_visible_seconds\"] - stats[\"total_time_without_other_seconds\"]) /\n",
        "                                    stats[\"total_time_visible_seconds\"] * 100, 1) if stats[\"total_time_visible_seconds\"] > 0 else 0,\n",
        "            \"filtered_out_exercises\": stats[\"filtered_out_exercises\"],\n",
        "            \"filtered_out_exercises_list\": \", \".join(stats[\"filtered_out_exercises_list\"]) if stats[\"filtered_out_exercises_list\"] else \"нет\"\n",
        "        }\n",
        "\n",
        "        # Добавляем информацию по каждому упражнению\n",
        "        for exercise, exercise_stats in stats[\"exercises\"].items():\n",
        "            row[f\"{exercise}_time_seconds\"] = exercise_stats[\"time_seconds\"]\n",
        "            row[f\"{exercise}_percentage\"] = exercise_stats[\"percentage\"]\n",
        "            row[f\"{exercise}_segments\"] = exercise_stats[\"segments\"]\n",
        "\n",
        "        df_rows.append(row)\n",
        "\n",
        "    # Создаем DataFrame\n",
        "    df = pd.DataFrame(df_rows)\n",
        "\n",
        "    # Заполняем NaN нулями для упражнений, которые были не у всех\n",
        "    df = df.fillna(0)\n",
        "\n",
        "    return statistics, df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQxkbPIJjvPb"
      },
      "outputs": [],
      "source": [
        "def save_statistics_to_csv(tracks, predictions, output_path=\"track_statistics.csv\"):\n",
        "    \"\"\"\n",
        "    Сохраняет статистику по трекам в CSV файл\n",
        "\n",
        "    Параметры:\n",
        "    -----------\n",
        "    tracks : dict\n",
        "        Словарь с треками\n",
        "    predictions : dict\n",
        "        Словарь с предсказаниями\n",
        "    output_path : str\n",
        "        Путь для сохранения CSV файла\n",
        "\n",
        "    Возвращает:\n",
        "    -----------\n",
        "    DataFrame : таблица со статистикой\n",
        "    \"\"\"\n",
        "\n",
        "    # Рассчитываем статистику\n",
        "    statistics, df = calculate_track_statistics(tracks, predictions)\n",
        "\n",
        "    # Сохраняем в CSV\n",
        "    df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
        "\n",
        "    # Выводим сводную информацию\n",
        "    print(f\"✅ Статистика сохранена в: {output_path}\")\n",
        "    print(f\"📊 Всего треков: {len(statistics)}\")\n",
        "\n",
        "    return df, statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODptKnb6jw4Y"
      },
      "outputs": [],
      "source": [
        "def create_statistics_report(statistics, df, output_dir=\"statistics_report\"):\n",
        "    \"\"\"\n",
        "    Создает и сохраняет отдельные графики с анализом статистики\n",
        "\n",
        "    Параметры:\n",
        "    -----------\n",
        "    tracks : dict\n",
        "        Словарь с треками\n",
        "    predictions : dict\n",
        "        Словарь с предсказаниями\n",
        "    output_dir : str\n",
        "        Папка для сохранения графиков\n",
        "\n",
        "    Возвращает:\n",
        "    -----------\n",
        "    dict : пути к сохраненным файлам\n",
        "    \"\"\"\n",
        "\n",
        "    # Создаем папку для отчетов\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Список для хранения путей к файлам\n",
        "    saved_files = {}\n",
        "\n",
        "    # Настройка стиля графиков\n",
        "    plt.style.use('seaborn-v0_8-darkgrid')\n",
        "    sns.set_palette(\"husl\")\n",
        "\n",
        "    # 1. График времени видимости по трекам\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    ax = sns.barplot(x='track_id', y='total_time_visible_seconds', data=df)\n",
        "    ax.set_title('Время видимости по трекам', fontsize=14, fontweight='bold')\n",
        "    ax.set_xlabel('Track ID', fontsize=12)\n",
        "    ax.set_ylabel('Секунды', fontsize=12)\n",
        "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
        "\n",
        "    # Добавляем значения на столбцы\n",
        "    for i, v in enumerate(df['total_time_visible_seconds']):\n",
        "        ax.text(i, v + 0.5, f'{v:.1f}', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    time_chart_path = os.path.join(output_dir, \"01_time_by_track.png\")\n",
        "    plt.savefig(time_chart_path, dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    saved_files[\"time_by_track\"] = time_chart_path\n",
        "    print(f\"✅ Сохранен график: {time_chart_path}\")\n",
        "\n",
        "    # 2. График количества уникальных упражнений по трекам\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    ax = sns.barplot(x='track_id', y='num_unique_exercises', data=df)\n",
        "    ax.set_title('Количество уникальных упражнений по трекам', fontsize=14, fontweight='bold')\n",
        "    ax.set_xlabel('Track ID', fontsize=12)\n",
        "    ax.set_ylabel('Количество упражнений', fontsize=12)\n",
        "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
        "\n",
        "    for i, v in enumerate(df['num_unique_exercises']):\n",
        "        ax.text(i, v + 0.1, f'{v}', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    exercises_chart_path = os.path.join(output_dir, \"02_exercises_by_track.png\")\n",
        "    plt.savefig(exercises_chart_path, dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    saved_files[\"exercises_by_track\"] = exercises_chart_path\n",
        "    print(f\"✅ Сохранен график: {exercises_chart_path}\")\n",
        "\n",
        "    # 3. График процента детекции\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    ax = sns.barplot(x='track_id', y='detection_percentage', data=df)\n",
        "    ax.set_title('Процент детекции по трекам', fontsize=14, fontweight='bold')\n",
        "    ax.set_xlabel('Track ID', fontsize=12)\n",
        "    ax.set_ylabel('Процент детекции (%)', fontsize=12)\n",
        "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
        "    ax.axhline(y=80, color='r', linestyle='--', alpha=0.5, label='Порог 80%')\n",
        "\n",
        "    for i, v in enumerate(df['detection_percentage']):\n",
        "        ax.text(i, v + 1, f'{v:.1f}%', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    detection_chart_path = os.path.join(output_dir, \"03_detection_percentage.png\")\n",
        "    plt.savefig(detection_chart_path, dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    saved_files[\"detection_percentage\"] = detection_chart_path\n",
        "    print(f\"✅ Сохранен график: {detection_chart_path}\")\n",
        "\n",
        "    # 4. Тепловая карта упражнений по трекам\n",
        "    # Собираем данные для тепловой карты\n",
        "    heatmap_data = []\n",
        "    all_exercises = set()\n",
        "\n",
        "    for track_id, stats in statistics.items():\n",
        "        for exercise, ex_stats in stats[\"exercises\"].items():\n",
        "            heatmap_data.append({\n",
        "                'track_id': track_id,\n",
        "                'exercise': exercise,\n",
        "                'time': ex_stats[\"time_seconds\"]\n",
        "            })\n",
        "            all_exercises.add(exercise)\n",
        "\n",
        "    if heatmap_data and all_exercises:\n",
        "        heatmap_df = pd.DataFrame(heatmap_data)\n",
        "        heatmap_pivot = heatmap_df.pivot_table(\n",
        "            index='track_id',\n",
        "            columns='exercise',\n",
        "            values='time',\n",
        "            aggfunc='sum',\n",
        "            fill_value=0\n",
        "        )\n",
        "\n",
        "        # Сортируем упражнения по общему времени\n",
        "        exercise_order = heatmap_pivot.sum().sort_values(ascending=False).index\n",
        "        heatmap_pivot = heatmap_pivot[exercise_order]\n",
        "\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        ax = sns.heatmap(\n",
        "            heatmap_pivot,\n",
        "            annot=True,\n",
        "            fmt='.1f',\n",
        "            cmap='YlOrRd',\n",
        "            cbar_kws={'label': 'Время (секунды)'},\n",
        "            linewidths=0.5\n",
        "        )\n",
        "        ax.set_title('Тепловая карта: время упражнений по трекам', fontsize=14, fontweight='bold')\n",
        "        ax.set_xlabel('Упражнения', fontsize=12)\n",
        "        ax.set_ylabel('Track ID', fontsize=12)\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        heatmap_path = os.path.join(output_dir, \"04_exercises_heatmap.png\")\n",
        "        plt.savefig(heatmap_path, dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        saved_files[\"exercises_heatmap\"] = heatmap_path\n",
        "        print(f\"✅ Сохранен график: {heatmap_path}\")\n",
        "\n",
        "    # 5. Круговая диаграмма распределения упражнений (общая)\n",
        "    if heatmap_data:\n",
        "        exercise_totals = heatmap_df.groupby('exercise')['time'].sum()\n",
        "        exercise_totals = exercise_totals[exercise_totals > 0]\n",
        "\n",
        "        if len(exercise_totals) > 0:\n",
        "            plt.figure(figsize=(10, 8))\n",
        "            colors = plt.cm.Set3(np.linspace(0, 1, len(exercise_totals)))\n",
        "\n",
        "            wedges, texts, autotexts = plt.pie(\n",
        "                exercise_totals.values,\n",
        "                labels=exercise_totals.index,\n",
        "                autopct='%1.1f%%',\n",
        "                startangle=90,\n",
        "                colors=colors,\n",
        "                pctdistance=0.85\n",
        "            )\n",
        "\n",
        "            plt.title('Распределение времени по упражнениям (все треки)', fontsize=14, fontweight='bold')\n",
        "\n",
        "            # Улучшаем читаемость\n",
        "            for autotext in autotexts:\n",
        "                autotext.set_color('black')\n",
        "                autotext.set_fontsize(10)\n",
        "                autotext.set_fontweight('bold')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            pie_chart_path = os.path.join(output_dir, \"05_exercises_distribution.png\")\n",
        "            plt.savefig(pie_chart_path, dpi=150, bbox_inches='tight')\n",
        "            plt.close()\n",
        "            saved_files[\"exercises_distribution\"] = pie_chart_path\n",
        "            print(f\"✅ Сохранен график: {pie_chart_path}\")\n",
        "\n",
        "    # 6. Stacked bar chart: время с упражнениями vs \"other\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Подготовка данных\n",
        "    track_ids = df['track_id'].astype(str)\n",
        "    time_without_other = df['total_time_without_other_seconds']\n",
        "    time_other = df['time_other_seconds']\n",
        "\n",
        "    p1 = plt.bar(track_ids, time_without_other, label='Время с упражнениями')\n",
        "    p2 = plt.bar(track_ids, time_other, bottom=time_without_other, label='Время \"other\"')\n",
        "\n",
        "    plt.title('Соотношение времени: упражнения vs \"other\"', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Track ID', fontsize=12)\n",
        "    plt.ylabel('Секунды', fontsize=12)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.legend()\n",
        "\n",
        "    # Добавляем общее время сверху\n",
        "    for i, (total, wo) in enumerate(zip(df['total_time_visible_seconds'], time_without_other)):\n",
        "        plt.text(i, total + 0.5, f'{total:.1f}', ha='center', va='bottom', fontsize=9)\n",
        "        # Процент времени с упражнениями\n",
        "        percentage = (wo / total * 100) if total > 0 else 0\n",
        "        plt.text(i, wo/2, f'{percentage:.0f}%', ha='center', va='center',\n",
        "                color='white', fontweight='bold', fontsize=9)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    stacked_chart_path = os.path.join(output_dir, \"06_time_distribution.png\")\n",
        "    plt.savefig(stacked_chart_path, dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    saved_files[\"time_distribution\"] = stacked_chart_path\n",
        "    print(f\"✅ Сохранен график: {stacked_chart_path}\")\n",
        "\n",
        "    # 7. График количества сегментов по трекам\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    ax = sns.barplot(x='track_id', y='total_segments', data=df)\n",
        "    ax.set_title('Количество сегментов по трекам', fontsize=14, fontweight='bold')\n",
        "    ax.set_xlabel('Track ID', fontsize=12)\n",
        "    ax.set_ylabel('Количество сегментов', fontsize=12)\n",
        "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
        "\n",
        "    for i, v in enumerate(df['total_segments']):\n",
        "        ax.text(i, v + 0.1, f'{v}', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    segments_chart_path = os.path.join(output_dir, \"07_segments_by_track.png\")\n",
        "    plt.savefig(segments_chart_path, dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    saved_files[\"segments_by_track\"] = segments_chart_path\n",
        "    print(f\"✅ Сохранен график: {segments_chart_path}\")\n",
        "\n",
        "    # 8. Box plot времени видимости\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.boxplot(y=df['total_time_visible_seconds'])\n",
        "    plt.title('Распределение времени видимости треков', fontsize=14, fontweight='bold')\n",
        "    plt.ylabel('Секунды', fontsize=12)\n",
        "\n",
        "    # Добавляем среднее значение\n",
        "    mean_time = df['total_time_visible_seconds'].mean()\n",
        "    plt.axhline(y=mean_time, color='r', linestyle='--', alpha=0.7,\n",
        "                label=f'Среднее: {mean_time:.1f} сек')\n",
        "\n",
        "    # Добавляем точки для каждого трека\n",
        "    for i, time in enumerate(df['total_time_visible_seconds']):\n",
        "        plt.scatter(0, time, alpha=0.6, s=50)\n",
        "\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    boxplot_path = os.path.join(output_dir, \"08_time_distribution_boxplot.png\")\n",
        "    plt.savefig(boxplot_path, dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    saved_files[\"time_boxplot\"] = boxplot_path\n",
        "    print(f\"✅ Сохранен график: {boxplot_path}\")\n",
        "\n",
        "    return saved_files, df, statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4Bc2W9DkNad",
        "outputId": "bacee54d-6f57-483a-b4f4-83fc83088dc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 Фильтрация треков: минимум 3 детекции (сегмента)\n",
            "  Сохранен трек 1: 20 сегментов\n",
            "  Сохранен трек 5: 6 сегментов\n",
            "  Пропущен трек 7: только 0 сегментов\n",
            "  Пропущен трек 9: только 1 сегментов\n",
            "  Пропущен трек 20: только 0 сегментов\n",
            "  Пропущен трек 21: только 1 сегментов\n",
            "📊 После фильтрации треков: 2 из 6 треков\n",
            "  Трек 1: упражнение 'leg raises' - 7 сегментов\n",
            "  Трек 1: упражнение 'tricep Pushdown' - 4 сегментов\n",
            "  Трек 1: упражнение 'tricep dips' - 4 сегментов\n",
            "  Трек 1: пропущены упражнения ['chest fly machine', 'lateral raise', 'leg extension', 'barbell biceps curl', 'other'] (≤2 сегментов)\n",
            "  Трек 5: упражнение 'pull Up' - 4 сегментов\n",
            "  Трек 5: пропущены упражнения ['other'] (≤2 сегментов)\n",
            "✅ Статистика сохранена в: track_statistics.csv\n",
            "📊 Всего треков: 2\n",
            "✅ Сохранен график: statistics_report/01_time_by_track.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3467851233.py:35: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
            "  ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
            "/tmp/ipython-input-3467851233.py:54: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
            "  ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Сохранен график: statistics_report/02_exercises_by_track.png\n",
            "✅ Сохранен график: statistics_report/03_detection_percentage.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3467851233.py:72: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
            "  ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Сохранен график: statistics_report/04_exercises_heatmap.png\n",
            "✅ Сохранен график: statistics_report/05_exercises_distribution.png\n",
            "✅ Сохранен график: statistics_report/06_time_distribution.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3467851233.py:206: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
            "  ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Сохранен график: statistics_report/07_segments_by_track.png\n",
            "✅ Сохранен график: statistics_report/08_time_distribution_boxplot.png\n"
          ]
        }
      ],
      "source": [
        "df, statistics = save_statistics_to_csv(tracks, predictions, \"track_statistics.csv\")\n",
        "\n",
        "saved_files, _, _ = create_statistics_report(\n",
        "    statistics, df,\n",
        "    output_dir=\"statistics_report\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAhcux-QSNoG"
      },
      "source": [
        "### Визуализация"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6uX3joepJ9F"
      },
      "outputs": [],
      "source": [
        "def visualize_with_exercises(video_path, tracks, predictions, output_path=\"tracking_with_exercises.mp4\"):\n",
        "    \"\"\"\n",
        "    Визуализация: скелеты разными цветами + подписи упражнений\n",
        "    \"\"\"\n",
        "    import cv2\n",
        "    import numpy as np\n",
        "    from tqdm import tqdm\n",
        "    import os\n",
        "\n",
        "    # Открываем видео\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Ошибка: не удалось открыть видео {video_path}\")\n",
        "        return\n",
        "\n",
        "    # Параметры видео\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    print(f\"Создание визуализации: {width}x{height}, FPS: {fps:.1f}\")\n",
        "\n",
        "    # Создаем VideoWriter\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    # Цвета для разных треков (BGR формат в OpenCV)\n",
        "    colors = [\n",
        "        (0, 255, 0),    # зеленый\n",
        "        (255, 0, 0),    # синий\n",
        "        (0, 0, 255),    # красный\n",
        "        (255, 255, 0),  # голубой\n",
        "        (255, 0, 255),  # розовый\n",
        "        (0, 255, 255),  # желтый\n",
        "        (255, 255, 255),# белый\n",
        "    ]\n",
        "\n",
        "    # Создаем карту кадр→треки\n",
        "    frame_map = {}\n",
        "    for track_id, track_data in tracks.items():\n",
        "        frame_indices = track_data.get(\"frame_indices\", [])\n",
        "        keypoints = track_data.get(\"keypoints_original\", [])\n",
        "\n",
        "        for i, frame_idx in enumerate(frame_indices):\n",
        "            if i < len(keypoints):\n",
        "                if frame_idx not in frame_map:\n",
        "                    frame_map[frame_idx] = []\n",
        "                frame_map[frame_idx].append((track_id, keypoints[i]))\n",
        "\n",
        "    # Обрабатываем видео\n",
        "    print(\"\\nОбработка видео...\")\n",
        "\n",
        "    for frame_idx in tqdm(range(total_frames), desc=\"Кадры\"):\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        current_time = frame_idx / fps if fps > 0 else 0\n",
        "\n",
        "        # Рисуем треки в текущем кадре\n",
        "        if frame_idx in frame_map:\n",
        "            for track_id, keypoints in frame_map[frame_idx]:\n",
        "                # Цвет для трека\n",
        "                color = colors[track_id % len(colors)]\n",
        "\n",
        "                # Рисуем скелет (упрощенный)\n",
        "                if len(keypoints) == 51:\n",
        "                    kp = keypoints.reshape(17, 3)\n",
        "                else:\n",
        "                    kp = keypoints\n",
        "\n",
        "                # Соединения\n",
        "                connections = [\n",
        "                    (0, 1), (0, 2), (1, 3), (2, 4),\n",
        "                    (5, 6), (5, 7), (7, 9), (6, 8), (8, 10),\n",
        "                    (11, 12), (11, 13), (13, 15), (12, 14), (14, 16),\n",
        "                    (5, 11), (6, 12)\n",
        "                ]\n",
        "\n",
        "                # Рисуем линии\n",
        "                for start_idx, end_idx in connections:\n",
        "                    if start_idx < len(kp) and end_idx < len(kp):\n",
        "                        x1, y1, c1 = kp[start_idx]\n",
        "                        x2, y2, c2 = kp[end_idx]\n",
        "\n",
        "                        if c1 > 0.3 and c2 > 0.3:\n",
        "                            if 0 <= x1 < width and 0 <= y1 < height and 0 <= x2 < width and 0 <= y2 < height:\n",
        "                                cv2.line(frame, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)\n",
        "\n",
        "                # Рисуем точки\n",
        "                for i, (x, y, conf) in enumerate(kp):\n",
        "                    if conf > 0.3 and 0 <= x < width and 0 <= y < height:\n",
        "                        cv2.circle(frame, (int(x), int(y)), 4, color, -1)\n",
        "\n",
        "                # Подпись с упражнением\n",
        "                exercise_text = f\"ID:{track_id}\"\n",
        "                if predictions and track_id in predictions:\n",
        "                    for segment in predictions[track_id]:\n",
        "                        if segment[\"start_time\"] <= current_time <= segment[\"end_time\"]:\n",
        "                            exercise_text = f\"ID:{track_id} - {segment['predicted_class']}\"\n",
        "                            break\n",
        "\n",
        "                # Позиция для текста\n",
        "                visible = kp[kp[:, 2] > 0.3]\n",
        "                if len(visible) > 0:\n",
        "                    text_x = int(np.mean(visible[:, 0]))\n",
        "                    text_y = int(np.min(visible[:, 1])) - 20\n",
        "\n",
        "                    text_x = max(10, min(text_x, width - 200))\n",
        "                    text_y = max(30, text_y)\n",
        "\n",
        "                    # Фон\n",
        "                    (w, h), _ = cv2.getTextSize(exercise_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
        "                    cv2.rectangle(frame,\n",
        "                                (text_x - 5, text_y - h - 5),\n",
        "                                (text_x + w + 5, text_y + 5),\n",
        "                                (0, 0, 0), -1)\n",
        "\n",
        "                    # Текст\n",
        "                    cv2.putText(frame, exercise_text,\n",
        "                              (text_x, text_y),\n",
        "                              cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
        "\n",
        "        # Время и номер кадра\n",
        "        time_str = f\"{int(current_time//60):02d}:{current_time%60:05.2f}\"\n",
        "        cv2.putText(frame, f\"Time: {time_str} | Frame: {frame_idx}/{total_frames}\",\n",
        "                   (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
        "\n",
        "        # Записываем кадр\n",
        "        out.write(frame)\n",
        "\n",
        "    # Освобождаем ресурсы\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    print(f\"\\n✅ Визуализация сохранена: {output_path}\")\n",
        "\n",
        "    return output_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ol4Egl_mpKdq",
        "outputId": "3aa712c5-2e54-4c5b-c743-bb5bf83cc0dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Создание визуализации: 1920x1080, FPS: 30.0\n",
            "\n",
            "Обработка видео...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Кадры: 100%|██████████| 1020/1020 [00:25<00:00, 39.49it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ Визуализация сохранена: exercise_tracking_final.mp4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "video_output = visualize_with_exercises(\n",
        "    video_path=\"/content/test_final.mp4\",\n",
        "    tracks=tracks,\n",
        "    predictions=predictions,\n",
        "    output_path=\"exercise_tracking_final.mp4\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMc86haLDFGu"
      },
      "source": [
        "### Тест пайплайна"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lT4_-SCtDHHg"
      },
      "outputs": [],
      "source": [
        "!unzip -q /content/workout_pipeline.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "10lmKA0AFClo",
        "outputId": "f72a05e1-9900-4c7c-aba5-8822430cc7fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from -r /content/workout_pipeline/requirements.txt (line 1)) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from -r /content/workout_pipeline/requirements.txt (line 2)) (0.24.0+cu126)\n",
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.12/dist-packages (from -r /content/workout_pipeline/requirements.txt (line 3)) (8.3.241)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (from -r /content/workout_pipeline/requirements.txt (line 4)) (4.12.0.88)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from -r /content/workout_pipeline/requirements.txt (line 5)) (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from -r /content/workout_pipeline/requirements.txt (line 6)) (4.67.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from -r /content/workout_pipeline/requirements.txt (line 7)) (6.0.3)\n",
            "Requirement already satisfied: lap>=0.5.12 in /usr/local/lib/python3.12/dist-packages (from -r /content/workout_pipeline/requirements.txt (line 8)) (0.5.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/workout_pipeline/requirements.txt (line 1)) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/workout_pipeline/requirements.txt (line 1)) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/workout_pipeline/requirements.txt (line 1)) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/workout_pipeline/requirements.txt (line 1)) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/workout_pipeline/requirements.txt (line 1)) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/workout_pipeline/requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/workout_pipeline/requirements.txt (line 1)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/workout_pipeline/requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/workout_pipeline/requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/workout_pipeline/requirements.txt (line 1)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/workout_pipeline/requirements.txt (line 1)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/workout_pipeline/requirements.txt (line 1)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/workout_pipeline/requirements.txt (line 1)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/workout_pipeline/requirements.txt (line 1)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/workout_pipeline/requirements.txt (line 1)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/workout_pipeline/requirements.txt (line 1)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/workout_pipeline/requirements.txt (line 1)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/workout_pipeline/requirements.txt (line 1)) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/workout_pipeline/requirements.txt (line 1)) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/workout_pipeline/requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/workout_pipeline/requirements.txt (line 1)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/workout_pipeline/requirements.txt (line 1)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r /content/workout_pipeline/requirements.txt (line 1)) (3.5.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->-r /content/workout_pipeline/requirements.txt (line 2)) (11.3.0)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics->-r /content/workout_pipeline/requirements.txt (line 3)) (3.10.0)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics->-r /content/workout_pipeline/requirements.txt (line 3)) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics->-r /content/workout_pipeline/requirements.txt (line 3)) (1.16.3)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics->-r /content/workout_pipeline/requirements.txt (line 3)) (5.9.5)\n",
            "Requirement already satisfied: polars>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics->-r /content/workout_pipeline/requirements.txt (line 3)) (1.31.0)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.18 in /usr/local/lib/python3.12/dist-packages (from ultralytics->-r /content/workout_pipeline/requirements.txt (line 3)) (2.0.18)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics->-r /content/workout_pipeline/requirements.txt (line 3)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics->-r /content/workout_pipeline/requirements.txt (line 3)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics->-r /content/workout_pipeline/requirements.txt (line 3)) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics->-r /content/workout_pipeline/requirements.txt (line 3)) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics->-r /content/workout_pipeline/requirements.txt (line 3)) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics->-r /content/workout_pipeline/requirements.txt (line 3)) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics->-r /content/workout_pipeline/requirements.txt (line 3)) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics->-r /content/workout_pipeline/requirements.txt (line 3)) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics->-r /content/workout_pipeline/requirements.txt (line 3)) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics->-r /content/workout_pipeline/requirements.txt (line 3)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics->-r /content/workout_pipeline/requirements.txt (line 3)) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->-r /content/workout_pipeline/requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->-r /content/workout_pipeline/requirements.txt (line 1)) (3.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics->-r /content/workout_pipeline/requirements.txt (line 3)) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -r /content/workout_pipeline/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4paZnoUDlkL",
        "outputId": "185a4e4a-1c55-4bd4-8223-f5d11f7cf2a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Загрузка модели из /content/best_pose_lstm_new.pth...\n",
            "Модель загружена на cuda\n",
            "Трекинг видео: /content/test_final.mp4\n",
            "\n",
            "Обработка человека ID 1\n",
            "  Сегментов: 16\n",
            "\n",
            "Обработка человека ID 5\n",
            "  Сегментов: 2\n",
            "\n",
            "Обработка человека ID 7\n",
            "  Сегментов: 0\n",
            "\n",
            "Обработка человека ID 8\n",
            "  Сегментов: 0\n",
            "✅ Tracks сохранены: output/2025-12-23_05-35-16/tracks.pkl\n",
            "✅ Predictions сохранены: output/2025-12-23_05-35-16/predictions.pkl\n",
            "\n",
            "Создание визуализации...\n",
            "Создание визуализации: 1920x1080, FPS: 30.0\n",
            "\n",
            "Обработка видео...\n",
            "Кадры: 100% 1020/1020 [00:21<00:00, 47.07it/s]\n",
            "\n",
            " Визуализация сохранена: output/2025-12-23_05-35-16/exercise_tracking_output.mp4\n",
            "✅ Видео сохранено: output/2025-12-23_05-35-16/exercise_tracking_output.mp4\n",
            "\n",
            "Расчет статистик и создание графиков...\n",
            "🔍 Фильтрация треков: минимум 3 детекции (сегмента)\n",
            "  Сохранен трек 1: 16 сегментов\n",
            "  Пропущен трек 5: только 2 сегментов\n",
            "  Пропущен трек 7: только 0 сегментов\n",
            "  Пропущен трек 8: только 0 сегментов\n",
            "📊 После фильтрации треков: 1 из 4 треков\n",
            "  Трек 1: упражнение 'leg raises' - 6 сегментов\n",
            "  Трек 1: упражнение 'tricep dips' - 4 сегментов\n",
            "  Трек 1: пропущены упражнения ['squat', 'tricep Pushdown', 'leg extension', 'barbell biceps curl', 'other'] (≤2 сегментов)\n",
            "✅ Статистика сохранена в: track_statistics.csv\n",
            "📊 Всего треков: 1\n",
            "/content/workout_pipeline/stats.py:248: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
            "  ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
            "✅ Сохранен график: output/2025-12-23_05-35-16/statistics_report/01_time_by_track.png\n",
            "/content/workout_pipeline/stats.py:267: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
            "  ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
            "✅ Сохранен график: output/2025-12-23_05-35-16/statistics_report/02_exercises_by_track.png\n",
            "/content/workout_pipeline/stats.py:285: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
            "  ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
            "✅ Сохранен график: output/2025-12-23_05-35-16/statistics_report/03_detection_percentage.png\n",
            "✅ Сохранен график: output/2025-12-23_05-35-16/statistics_report/04_exercises_heatmap.png\n",
            "✅ Сохранен график: output/2025-12-23_05-35-16/statistics_report/05_exercises_distribution.png\n",
            "✅ Сохранен график: output/2025-12-23_05-35-16/statistics_report/06_time_distribution.png\n",
            "/content/workout_pipeline/stats.py:419: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
            "  ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
            "✅ Сохранен график: output/2025-12-23_05-35-16/statistics_report/07_segments_by_track.png\n",
            "✅ Сохранен график: output/2025-12-23_05-35-16/statistics_report/08_time_distribution_boxplot.png\n",
            "✅ Графики сохранены по пути output/2025-12-23_05-35-16/statistics_report\n",
            "\n",
            "🎯 Обработка завершена!\n",
            "   - Tracks: output/2025-12-23_05-35-16/tracks.pkl\n",
            "   - Predictions: output/2025-12-23_05-35-16/predictions.pkl\n",
            "   - Video: output/2025-12-23_05-35-16/exercise_tracking_output.mp4\n",
            "   - Video: output/2025-12-23_05-35-16/statistics_report\n"
          ]
        }
      ],
      "source": [
        "!python /content/workout_pipeline/main.py --video_path /content/test_final.mp4 --model_path /content/best_pose_lstm_new.pth --visualize --stats"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
