import cv2
import numpy as np
import torch
import json
import pickle
import os
from datetime import datetime
from person_tracker import process_video_for_lstm
from pose_preprocessor import PosePreprocessor
from inference import load_lstm_model, classify_person_sequence
from visualization import visualize_with_exercises
from stats import save_statistics_to_csv, create_statistics_report

def full_process(video_path, model_path, min_visibility_sec=5, min_detection_percent=40):
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ
    """
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    # –ó–∞–≥—Ä—É–∂–∞–µ–º LSTM –º–æ–¥–µ–ª—å
    lstm_model = load_lstm_model(model_path, device)
    preprocessor = PosePreprocessor(seq_length=50, training=False)

    # 1. –¢—Ä–µ–∫–∏–Ω–≥ - –ø–æ–ª—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
    print(f"–¢—Ä–µ–∫–∏–Ω–≥ –≤–∏–¥–µ–æ: {video_path}")
    tracks = process_video_for_lstm(
        video_path=video_path,
        min_visibility_seconds=min_visibility_sec,
        min_detection_percent=min_detection_percent
    )
    
    clean_tracks = tracks
    if not tracks:
        print("–ù–µ—Ç –ª—é–¥–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        return {}, {}

    # 2. –î–ª—è –∫–∞–∂–¥–æ–≥–æ —á–µ–ª–æ–≤–µ–∫–∞ —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ 2-—Å–µ–∫—É–Ω–¥–Ω—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã
    predictions = {}

    for track_id, track_data in tracks.items():
        print(f"\n–û–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ–ª–æ–≤–µ–∫–∞ ID {track_id}")
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –û–†–ò–ì–ò–ù–ê–õ–¨–ù–´–ï –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
        keypoints_original = track_data["keypoints_original"]
        timestamps = track_data["timestamps"]

        if len(timestamps) < 2:
            continue

        # –ü–æ–ª—É—á–∞–µ–º FPS –≤–∏–¥–µ–æ
        cap = cv2.VideoCapture(video_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        if fps <= 0:
            fps = 30
        cap.release()

        window_frames = int(2 * fps)  # 2 —Å–µ–∫—É–Ω–¥—ã
        step_frames = int(2 * fps)    # 2 —Å–µ–∫—É–Ω–¥–∞ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è

        segments = []

        for start in range(0, len(keypoints_original) - window_frames + 1, step_frames):
            end = start + window_frames

            segment = {
                "track_id": track_id,
                "start_time": float(timestamps[start]),
                "end_time": float(timestamps[end-1]),
                "keypoints": keypoints_original[start:end],  # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
                "segment_idx": len(segments)
            }
            segments.append(segment)

        print(f"  –°–µ–≥–º–µ–Ω—Ç–æ–≤: {len(segments)}")

        # 3. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∫–∞–∂–¥–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞
        segment_predictions = []
        
        class_names = [
            'barbell biceps curl', 'bench press', 'chest fly machine', 'deadlift', 
            'decline bench press', 'hammer curl', 'hip thrust', 'incline bench press', 
            'lat pulldown', 'lateral raise', 'leg extension', 'leg raises', 'other', 
            'plank', 'pull Up', 'push-up', 'romanian deadlift', 'russian twist', 
            'shoulder press', 'squat', 't bar row', 'tricep Pushdown', 'tricep dips'
        ]
        
        for segment in segments:
            prediction = classify_person_sequence(
                segment=segment,
                lstm_model=lstm_model,
                preprocessor=preprocessor,
                class_names=class_names,
                device=device
            )

            result = {
                "track_id": track_id,
                "segment_idx": segment["segment_idx"],
                "start_time": segment["start_time"],
                "end_time": segment["end_time"],
                "predicted_class": prediction["predicted_class"],
                "confidence": prediction["confidence"],
                "class_idx": prediction["class_idx"]
            }
            segment_predictions.append(result)

        predictions[track_id] = segment_predictions
    
    return clean_tracks, predictions

def save_results(tracks, predictions, output_dir="output"):
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–∞–π–ª—ã
    """
    now = datetime.now()
    timestamp = now.strftime("%Y-%m-%d_%H-%M-%S")
    output_dir = os.path.join(output_dir, timestamp)
    
    os.makedirs(output_dir, exist_ok=True)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º tracks
    tracks_path = os.path.join(output_dir, "tracks.pkl")
    with open(tracks_path, 'wb') as f:
        pickle.dump(tracks, f)
    print(f"‚úÖ Tracks —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {tracks_path}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º predictions –≤ pickle
    predictions_path = os.path.join(output_dir, "predictions.pkl")
    with open(predictions_path, 'wb') as f:
        pickle.dump(predictions, f)
    print(f"‚úÖ Predictions —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {predictions_path}")
    
    return output_dir, tracks_path, predictions_path

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="Exercise Recognition Pipeline")
    parser.add_argument("--video_path", type=str, required=True, help="Path to input video")
    parser.add_argument("--model_path", type=str, required=True, help="Path to LSTM model weights")
    parser.add_argument("--visualize", action="store_true", help="Generate visualization video")
    parser.add_argument("--stats", action="store_true", help="Generate stats graphs")
    parser.add_argument("--output_dir", type=str, default="output", help="Output directory")
    
    args = parser.parse_args()
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ
    tracks, predictions = full_process(
        video_path=args.video_path,
        model_path=args.model_path,
        min_visibility_sec=2,
        min_detection_percent=60
    )
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    output_dir_with_date, tracks_path, predictions_path = save_results(tracks, predictions, args.output_dir)
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    if args.visualize:
        print("\n–°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏...")
        video_output = os.path.join(output_dir_with_date, "exercise_tracking_output.mp4")
        visualize_with_exercises(
            video_path=args.video_path,
            tracks=tracks,
            predictions=predictions,
            output_path=video_output
        )
        print(f"‚úÖ –í–∏–¥–µ–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {video_output}")
    
    if args.stats:
        print("\n–†–∞—Å—á–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤...")
        stats_output = os.path.join(output_dir_with_date, "statistics_report")

        df, statistics = save_statistics_to_csv(tracks, predictions, "track_statistics.csv")
        _, _, _ = create_statistics_report(statistics, df, output_dir=stats_output)
        print(f"‚úÖ –ì—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –ø–æ –ø—É—Ç–∏ {stats_output}")


    print(f"\nüéØ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    print(f"   - Tracks: {tracks_path}")
    print(f"   - Predictions: {predictions_path}")
    if args.visualize:
        print(f"   - Video: {video_output}")
    if args.stats:
        print(f"   - Stats: {stats_output}")

if __name__ == "__main__":
    main()