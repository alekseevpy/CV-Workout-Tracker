{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f233d81",
   "metadata": {},
   "source": [
    "### Установка зависимостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f46922b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Базовое обновление инструментов\n",
    "# %pip install -U pip setuptools wheel\n",
    "\n",
    "# # PyTorch\n",
    "# %pip install -U torch torchvision torchaudio\n",
    "\n",
    "# # Установщик OpenMMLab\n",
    "# %pip install -U openmim\n",
    "\n",
    "# # MMEngine и MMCV 2.1.x\n",
    "# !mim install \"mmengine>=0.10.0\"\n",
    "# !mim install \"mmcv==2.1.0\"\n",
    "\n",
    "# # MMDetection для авто-детекции людей 3.2.x\n",
    "# !mim install \"mmdet==3.2.0\"\n",
    "\n",
    "# # MMPose 1.x\n",
    "# !mim install \"mmpose==1.3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc5d301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U gdown imageio-ffmpeg --quiet\n",
    "# %pip install -q -U imageio imageio-ffmpeg\n",
    "\n",
    "# %pip install -U \"numpy==2.2.6\" cython\n",
    "\n",
    "# # снесём и пересоберём оба COCO-пакета\n",
    "# %pip uninstall -y xtcocotools pycocotools\n",
    "# %pip install --no-binary=xtcocotools,pycocotools --no-cache-dir xtcocotools pycocotools\n",
    "\n",
    "# # на всякий случай переустановим mmpose (поверх тех же версий)\n",
    "# %pip install -U --no-cache-dir \"mmpose==1.3.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d5c041",
   "metadata": {},
   "source": [
    "### Перезапускаем ядро, проверяем версии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf711b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platform: macOS-15.6.1-arm64-arm-64bit\n",
      "Python: 3.11.13 (main, Jun  3 2025, 18:38:25) [Clang 17.0.0 (clang-1700.0.13.3)]\n",
      "\n",
      "torch: 2.2.2\n",
      "torchvision: 0.17.2\n",
      "mmengine: 0.10.7\n",
      "mmcv: 2.1.0\n",
      "mmdet: 3.2.0\n",
      "\n",
      "MMPoseInferencer import OK\n",
      "mmpose: 1.3.2\n",
      "\n",
      "xtcocotools with numpy version: 1.26.4\n",
      "\n",
      "Device selected: mps\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import platform\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import xtcocotools._mask as _mask\n",
    "from mmpose.apis import MMPoseInferencer\n",
    "\n",
    "\n",
    "def _ver(pkg):\n",
    "    try:\n",
    "        m = importlib.import_module(pkg)\n",
    "        return getattr(m, \"__version__\", \"N/A\")\n",
    "    except Exception as e:\n",
    "        return f\"not installed ({e})\"\n",
    "\n",
    "\n",
    "print(\"Platform:\", platform.platform())\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"\\ntorch:\", _ver(\"torch\"))\n",
    "print(\"torchvision:\", _ver(\"torchvision\"))\n",
    "print(\"mmengine:\", _ver(\"mmengine\"))\n",
    "print(\"mmcv:\", _ver(\"mmcv\"))\n",
    "print(\"mmdet:\", _ver(\"mmdet\"))\n",
    "print(\"\\nMMPoseInferencer import OK\")\n",
    "print(\"mmpose:\", _ver(\"mmpose\"))\n",
    "print(\"\\nxtcocotools with numpy version:\", _ver(\"numpy\"))\n",
    "\n",
    "\n",
    "print(\n",
    "    \"\\nDevice selected:\", \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4c3675",
   "metadata": {},
   "source": [
    "### Каталоги проекта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd43c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR = /Users/pussykiller/Учеба/Семестр 3/Проектный практикум/CV-Workout-Tracker/notebooks/mmpose\n",
      "INPUT_DIR = /Users/pussykiller/Учеба/Семестр 3/Проектный практикум/CV-Workout-Tracker/notebooks/mmpose/input_compare_angles\n",
      "OUTPUT_DIR = /Users/pussykiller/Учеба/Семестр 3/Проектный практикум/CV-Workout-Tracker/notebooks/mmpose/output_compare_angles\n",
      "WEIGHTS_DIR = /Users/pussykiller/Учеба/Семестр 3/Проектный практикум/CV-Workout-Tracker/notebooks/mmpose/weights\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path(\".\").resolve()\n",
    "INPUT_DIR = BASE_DIR / \"input_compare_angles\"\n",
    "OUTPUT_DIR = BASE_DIR / \"output_compare_angles\"\n",
    "WEIGHTS_DIR = BASE_DIR / \"weights\"\n",
    "INPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "WEIGHTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"BASE_DIR =\", BASE_DIR)\n",
    "print(\"INPUT_DIR =\", INPUT_DIR)\n",
    "print(\"OUTPUT_DIR =\", OUTPUT_DIR)\n",
    "print(\"WEIGHTS_DIR =\", WEIGHTS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38541133",
   "metadata": {},
   "source": [
    "# Попытка на фото"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766d5e3f",
   "metadata": {},
   "source": [
    "### 1. Скачиваем изображение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2eed2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import gdown\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "\n",
    "GD_URL = \"https://drive.google.com/file/d/1RenekFPFYrB1UhAHhdZf67LeCUw8WbZh/view?usp=sharing\"\n",
    "img_path = INPUT_DIR / \"cheliki_na_turnike.jpg\"\n",
    "\n",
    "gdown.download(GD_URL, str(img_path), quiet=False, fuzzy=True)\n",
    "assert img_path.exists(), f\"Изображение не скачалось: {img_path}\"\n",
    "\n",
    "print(\"Скачано в:\", img_path)\n",
    "print(\"Размер: {:.2f} МБ\".format(img_path.stat().st_size / (1024**2)))\n",
    "\n",
    "display(Image.open(img_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed416c4",
   "metadata": {},
   "source": [
    "### 2. Применяем MMPoseInferencer на скачанном изображении"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a31248b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from mmpose.apis import MMPoseInferencer\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "try:\n",
    "    torch.serialization.add_safe_globals(\n",
    "        [\n",
    "            np.core.multiarray._reconstruct,\n",
    "            np.dtype,\n",
    "            np.ufunc,\n",
    "        ]\n",
    "    )\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(\"Используем изображение:\", img_path)\n",
    "\n",
    "# Инициализируем унифицированный инференсер MMPose\n",
    "# с alias \"human\" (детектор + оценка позы)\n",
    "inferencer = MMPoseInferencer(\"human\", device=device)\n",
    "\n",
    "result_gen = inferencer(\n",
    "    str(img_path),\n",
    "    return_vis=True,  # вернуть массив визуализации в result['visualization']\n",
    "    show=False,  # не открывать отдельное окно\n",
    "    radius=16,\n",
    "    thickness=8,  # сделать точки/скелет чуть заметнее\n",
    "    vis_out_dir=str(\n",
    "        OUTPUT_DIR / \"vis\"\n",
    "    ),  # сюда сохранятся картинки с разметкой\n",
    "    pred_out_dir=str(OUTPUT_DIR / \"pred\"),  # сюда — JSON с ключевыми точками\n",
    ")\n",
    "\n",
    "result = next(result_gen)\n",
    "\n",
    "vis_list = result.get(\"visualization\", [])\n",
    "if not vis_list:\n",
    "    raise RuntimeError(\n",
    "        \"Нет визуализации в результате. Убедитесь, что return_vis=True.\"\n",
    "    )\n",
    "\n",
    "vis_img = vis_list[\n",
    "    0\n",
    "]  # это RGB-изображение с нарисованными ключевыми точками/скелетом\n",
    "\n",
    "# 5) Сохраним и покажем\n",
    "out_file = OUTPUT_DIR / f\"{img_path.stem}_pose_vis.jpg\"\n",
    "# cv2.imwrite ожидает BGR -> конвертируем\n",
    "cv2.imwrite(str(out_file), vis_img[..., ::-1])\n",
    "\n",
    "print(\"Визуализация сохранена в:\", out_file)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(vis_img)  # vis_img уже в RGB\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"MMPose: скелетики на изображении\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8858486e",
   "metadata": {},
   "source": [
    "# Попытка на видео"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0e933a",
   "metadata": {},
   "source": [
    "### 1. Скачиваем видео и анализируем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c80d878",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1iDHBbYgV_sYRyVUi_BLdqcGogD_CDXmB\n",
      "To: /Users/pussykiller/Учеба/Семестр 3/Проектный практикум/CV-Workout-Tracker/notebooks/mmpose/input_compare_angles/tiktonik_360p.mp4\n",
      "100%|██████████| 2.30M/2.30M [00:00<00:00, 3.54MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Видео: /Users/pussykiller/Учеба/Семестр 3/Проектный практикум/CV-Workout-Tracker/notebooks/mmpose/input_compare_angles/tiktonik_360p.mp4\n",
      "Размер: 482x360, FPS: 30.000, кадров: 1185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import gdown\n",
    "\n",
    "GD_URL = \"https://drive.google.com/file/d/1iDHBbYgV_sYRyVUi_BLdqcGogD_CDXmB/view?usp=sharing\"\n",
    "video_pth = INPUT_DIR / \"tiktonik_360p.mp4\"\n",
    "\n",
    "video_pth.unlink(missing_ok=True)\n",
    "gdown.download(GD_URL, str(video_pth), quiet=False, fuzzy=True)\n",
    "assert video_pth.exists(), f\"Видео не скачалось: {video_pth}\"\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(str(video_pth))\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(f\"Не удалось открыть видео: {video_pth}\")\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) or 25.0\n",
    "w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH) or 0)\n",
    "h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT) or 0)\n",
    "n = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
    "cap.release()\n",
    "\n",
    "print(f\"Видео: {video_pth}\")\n",
    "print(f\"Размер: {w}x{h}, FPS: {fps:.3f}, кадров: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8de363",
   "metadata": {},
   "source": [
    "### 2. Применяем MMPoseInferencer на скачанном видео"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3168d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by http backend from path: https://download.openmmlab.com/mmpose/v1/projects/rtmposev1/rtmpose-m_simcc-body7_pt-body7_420e-256x192-e48f03d0_20230504.pth\n",
      "11/14 11:33:53 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Failed to search registry with scope \"mmpose\" in the \"function\" registry tree. As a workaround, the current \"function\" registry in \"mmengine\" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether \"mmpose\" is a correct scope, or whether the registry is initialized.\n",
      "Loads checkpoint by http backend from path: https://download.openmmlab.com/mmpose/v1/projects/rtmposev1/rtmdet_m_8xb32-100e_coco-obj365-person-235e8209.pth\n",
      "11/14 11:33:53 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Failed to search registry with scope \"mmdet\" in the \"function\" registry tree. As a workaround, the current \"function\" registry in \"mmengine\" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether \"mmdet\" is a correct scope, or whether the registry is initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pussykiller/Учеба/Семестр 3/Проектный практикум/CV-Workout-Tracker/venv311/lib/python3.11/site-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3550.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (482, 360) to (496, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "[vost#0:0/libx264 @ 0x1229054c0] Multiple -pix_fmt options specified for stream 0, only the last option '-pix_fmt yuv420p' will be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обработано кадров: 50\n",
      "Обработано кадров: 100\n",
      "Обработано кадров: 150\n",
      "Обработано кадров: 200\n",
      "Обработано кадров: 250\n",
      "Обработано кадров: 300\n",
      "Обработано кадров: 350\n",
      "Обработано кадров: 400\n",
      "Обработано кадров: 450\n",
      "Обработано кадров: 500\n",
      "Обработано кадров: 550\n",
      "Обработано кадров: 600\n",
      "Обработано кадров: 650\n",
      "Обработано кадров: 700\n",
      "Обработано кадров: 750\n",
      "Обработано кадров: 800\n",
      "Обработано кадров: 850\n",
      "Обработано кадров: 900\n",
      "Обработано кадров: 950\n",
      "Обработано кадров: 1000\n",
      "Обработано кадров: 1050\n",
      "Обработано кадров: 1100\n",
      "Обработано кадров: 1150\n",
      "Готово! Сохранено кадров: 1185\n",
      "Выходной файл: /Users/pussykiller/Учеба/Семестр 3/Проектный практикум/CV-Workout-Tracker/notebooks/mmpose/output_compare_angles/tiktonik_pose/visualization/tiktonik_360p_pose.mp4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import imageio\n",
    "import numpy as np\n",
    "import torch\n",
    "from mmpose.apis import MMPoseInferencer\n",
    "\n",
    "# Пути и устройство\n",
    "device = \"cpu\"  # на macOS так избегаем проблемы NMS на MPS\n",
    "out_dir = OUTPUT_DIR / \"tiktonik_pose\"\n",
    "\n",
    "# Инициализируем единый инференсер с alias \"human\"\n",
    "# (под капотом подтянет RTMDet для людей + 2D-позу; умеет видео/изображения;\n",
    "# настраиваемые radius/thickness)\n",
    "inferencer = MMPoseInferencer(\n",
    "    \"human\",\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Готовим writer с кодеком H.264\n",
    "vis_dir = out_dir / \"visualization\"\n",
    "vis_dir.mkdir(parents=True, exist_ok=True)\n",
    "out_video = vis_dir / (video_pth.stem + \"_pose.mp4\")\n",
    "\n",
    "writer = imageio.get_writer(\n",
    "    out_video.as_posix(),\n",
    "    fps=fps,\n",
    "    codec=\"libx264\",\n",
    "    format=\"FFMPEG\",\n",
    "    output_params=[\"-pix_fmt\", \"yuv420p\"],\n",
    ")\n",
    "\n",
    "# Запускаем ленивый генератор инференса по видео.\n",
    "# Чтобы получать кадры с отрисовкой в Python, включаем return_vis=True\n",
    "# и задаем толщину/радиус. При желании можно включить/выключить рамки:\n",
    "# draw_bbox=True/False.\n",
    "result_gen = inferencer(\n",
    "    str(video_pth),\n",
    "    show=False,\n",
    "    return_vis=True,\n",
    "    radius=12,\n",
    "    thickness=6,\n",
    "    draw_bbox=False,\n",
    ")\n",
    "\n",
    "# Пробегаем все результаты и пишем в MP4\n",
    "frames_written = 0\n",
    "for res in result_gen:\n",
    "    # 1) Заберём визуализацию (может быть списком или None)\n",
    "    vis = res.get(\"visualization\")\n",
    "    if isinstance(vis, list):\n",
    "        vis = vis[0] if vis else None\n",
    "\n",
    "    # 2) Если отрисовка вернулась путём (бывает), читаем с диска\n",
    "    if vis is None:\n",
    "        vis_path = res.get(\"visualization_path\")\n",
    "        if isinstance(vis_path, list):\n",
    "            vis_path = vis_path[0] if vis_path else None\n",
    "        if vis_path:\n",
    "            vis = imageio.v2.imread(vis_path)  # RGB\n",
    "\n",
    "    if vis is None:\n",
    "        continue  # нечего писать\n",
    "\n",
    "    # 3) Приводим к RGB uint8 HxWx3\n",
    "    frame = np.asarray(vis)\n",
    "    if frame.ndim == 2:  # если ч/б\n",
    "        frame = np.repeat(frame[..., None], 3, axis=2)\n",
    "    elif (\n",
    "        frame.ndim == 3 and frame.shape[2] == 4\n",
    "    ):  # если RGBA — отбрасываем альфу\n",
    "        frame = frame[..., :3]\n",
    "    if frame.dtype != np.uint8:\n",
    "        frame = np.clip(frame, 0, 255).astype(np.uint8)\n",
    "\n",
    "    # 4) Подгоним размер под исходное видео (на всякий)\n",
    "    if frame.shape[1] != w or frame.shape[0] != h:\n",
    "        frame = cv2.resize(frame, (w, h), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    # 5) Пишем в MP4 (imageio ожидает RGB)\n",
    "    writer.append_data(frame)\n",
    "    frames_written += 1\n",
    "    if frames_written % 50 == 0:\n",
    "        print(f\"Обработано кадров: {frames_written}\")\n",
    "\n",
    "writer.close()\n",
    "print(f\"Готово! Сохранено кадров: {frames_written}\")\n",
    "print(\"Выходной файл:\", out_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9551c250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras import Model, layers, utils\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    ReduceLROnPlateau,\n",
    ")\n",
    "from tensorflow.keras.layers import (\n",
    "    AveragePooling2D,\n",
    "    BatchNormalization,\n",
    "    Conv2D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    GaussianDropout,\n",
    "    GlobalAveragePooling2D,\n",
    "    Input,\n",
    "    LeakyReLU,\n",
    "    MaxPooling2D,\n",
    "    SpatialDropout2D,\n",
    ")\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.regularizers import *\n",
    "from tensorflow.random import set_seed\n",
    "\n",
    "\n",
    "def seed_everything(seed):\n",
    "    np.random.seed(seed)\n",
    "    set_seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56585b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTO = tf.data.AUTOTUNE\n",
    "\n",
    "# GPU settings\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "print(\"GPUs:\", gpus)\n",
    "if gpus:\n",
    "    try:\n",
    "        for g in gpus:\n",
    "            tf.config.experimental.set_memory_growth(g, True)\n",
    "    except Exception as e:\n",
    "        print(\"mem growth err:\", e)\n",
    "\n",
    "# mixed precision ускоряет на T4/P100\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "print(\"policy:\", mixed_precision.global_policy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cb7ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "GCS_DS_PATH = KaggleDatasets().get_gcs_path()\n",
    "\n",
    "IMAGE_SIZE = [331, 331]  # ключевое отличие от 224\n",
    "NUM_CLASSES = 104\n",
    "EPOCHS_STAGE1 = 4\n",
    "EPOCHS_STAGE2 = 12\n",
    "EPOCHS_STAGE3 = 8\n",
    "\n",
    "BATCH_SIZE = 16  # для 331 на T4 обычно норм; если OOM -> 8\n",
    "\n",
    "GCS_PATH_SELECT = {\n",
    "    192: GCS_DS_PATH + \"/tfrecords-jpeg-192x192\",\n",
    "    224: GCS_DS_PATH + \"/tfrecords-jpeg-224x224\",\n",
    "    331: GCS_DS_PATH + \"/tfrecords-jpeg-331x331\",\n",
    "    512: GCS_DS_PATH + \"/tfrecords-jpeg-512x512\",\n",
    "}\n",
    "GCS_PATH = GCS_PATH_SELECT[IMAGE_SIZE[0]]\n",
    "\n",
    "TRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH + \"/train/*.tfrec\")\n",
    "VALIDATION_FILENAMES = tf.io.gfile.glob(GCS_PATH + \"/val/*.tfrec\")\n",
    "TEST_FILENAMES = tf.io.gfile.glob(GCS_PATH + \"/test/*.tfrec\")\n",
    "\n",
    "\n",
    "def count_samples(filenames):\n",
    "    counts = [int(re.search(r\"-([0-9]+)\\.\", f).group(1)) for f in filenames]\n",
    "    return int(np.sum(counts))\n",
    "\n",
    "\n",
    "NUM_TRAINING_IMAGES = count_samples(TRAINING_FILENAMES)\n",
    "NUM_VALIDATION_IMAGES = count_samples(VALIDATION_FILENAMES)\n",
    "NUM_TEST_IMAGES = count_samples(TEST_FILENAMES)\n",
    "\n",
    "STEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\n",
    "VAL_STEPS = math.ceil(NUM_VALIDATION_IMAGES / BATCH_SIZE)\n",
    "\n",
    "print(\n",
    "    \"train/val/test:\",\n",
    "    NUM_TRAINING_IMAGES,\n",
    "    NUM_VALIDATION_IMAGES,\n",
    "    NUM_TEST_IMAGES,\n",
    ")\n",
    "print(\"steps:\", STEPS_PER_EPOCH, \"val_steps:\", VAL_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9c67cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_and_resize(image_data):\n",
    "    img = tf.image.decode_jpeg(image_data, channels=3)\n",
    "    img = tf.image.resize(img, IMAGE_SIZE, method=\"bilinear\")\n",
    "    img = tf.cast(img, tf.float32) / 255.0\n",
    "    return img\n",
    "\n",
    "\n",
    "def read_labeled(example):\n",
    "    features = tf.io.parse_single_example(\n",
    "        example,\n",
    "        {\n",
    "            \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "            \"class\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        },\n",
    "    )\n",
    "    return decode_and_resize(features[\"image\"]), tf.cast(\n",
    "        features[\"class\"], tf.int32\n",
    "    )\n",
    "\n",
    "\n",
    "def read_unlabeled(example):\n",
    "    features = tf.io.parse_single_example(\n",
    "        example,\n",
    "        {\n",
    "            \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "            \"id\": tf.io.FixedLenFeature([], tf.string),\n",
    "        },\n",
    "    )\n",
    "    return decode_and_resize(features[\"image\"]), features[\"id\"]\n",
    "\n",
    "\n",
    "# Keras-aug слои (быстро на GPU)\n",
    "augmenter = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "        tf.keras.layers.RandomRotation(0.10),\n",
    "        tf.keras.layers.RandomZoom(0.10),\n",
    "        tf.keras.layers.RandomTranslation(0.06, 0.06),\n",
    "        tf.keras.layers.RandomContrast(0.15),\n",
    "    ],\n",
    "    name=\"augmenter\",\n",
    ")\n",
    "\n",
    "\n",
    "def augment(image, label):\n",
    "    image = augmenter(image, training=True)\n",
    "    image = tf.clip_by_value(image, 0.0, 1.0)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "def build_dataset(filenames, labeled=True, ordered=False, do_augment=False):\n",
    "    opt = tf.data.Options()\n",
    "    opt.experimental_deterministic = ordered\n",
    "    ds = tf.data.TFRecordDataset(\n",
    "        filenames, num_parallel_reads=AUTO\n",
    "    ).with_options(opt)\n",
    "    ds = ds.map(\n",
    "        read_labeled if labeled else read_unlabeled, num_parallel_calls=AUTO\n",
    "    )\n",
    "    if labeled and do_augment:\n",
    "        ds = ds.map(augment, num_parallel_calls=AUTO)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def get_train_ds():\n",
    "    ds = build_dataset(\n",
    "        TRAINING_FILENAMES, labeled=True, ordered=False, do_augment=True\n",
    "    )\n",
    "    ds = ds.shuffle(2048, seed=SEED).repeat()\n",
    "    ds = ds.batch(BATCH_SIZE, drop_remainder=True).prefetch(AUTO)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def get_val_ds():\n",
    "    ds = build_dataset(\n",
    "        VALIDATION_FILENAMES, labeled=True, ordered=True, do_augment=False\n",
    "    )\n",
    "    ds = ds.batch(BATCH_SIZE, drop_remainder=False).cache().prefetch(AUTO)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def get_test_ds():\n",
    "    ds = build_dataset(\n",
    "        TEST_FILENAMES, labeled=False, ordered=True, do_augment=False\n",
    "    )\n",
    "    ds = ds.batch(BATCH_SIZE, drop_remainder=False).prefetch(AUTO)\n",
    "    return ds\n",
    "\n",
    "\n",
    "val_ds = get_val_ds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd59c7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MacroF1(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, val_ds):\n",
    "        super().__init__()\n",
    "        self.val_ds = val_ds\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        y_true, y_pred = [], []\n",
    "        for x, y in self.val_ds:\n",
    "            p = self.model.predict_on_batch(x)\n",
    "            y_pred.append(np.argmax(p, axis=1))\n",
    "            y_true.append(y.numpy())\n",
    "        y_true = np.concatenate(y_true)\n",
    "        y_pred = np.concatenate(y_pred)\n",
    "        score = f1_score(y_true, y_pred, average=\"macro\")\n",
    "        logs[\"val_macro_f1\"] = score\n",
    "        print(f\"\\nval_macro_f1: {score:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffae82e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    inp = tf.keras.Input(shape=(*IMAGE_SIZE, 3))\n",
    "    x = inp * 255.0\n",
    "    x = tf.keras.applications.efficientnet.preprocess_input(x)\n",
    "\n",
    "    base = tf.keras.applications.EfficientNetB3(\n",
    "        include_top=False, weights=\"imagenet\", input_shape=(*IMAGE_SIZE, 3)\n",
    "    )\n",
    "    base.trainable = False\n",
    "\n",
    "    x = base(x, training=False)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dropout(0.35)(x)\n",
    "    out = tf.keras.layers.Dense(\n",
    "        NUM_CLASSES, activation=\"softmax\", dtype=\"float32\"\n",
    "    )(x)\n",
    "    model = tf.keras.Model(inp, out)\n",
    "    return model, base\n",
    "\n",
    "\n",
    "def cosine_lr(max_lr, min_lr, total_steps):\n",
    "    # простая cosine decay без warmup\n",
    "    def schedule(step):\n",
    "        t = tf.cast(step, tf.float32) / tf.cast(total_steps, tf.float32)\n",
    "        lr = min_lr + 0.5 * (max_lr - min_lr) * (1 + tf.cos(np.pi * t))\n",
    "        return lr\n",
    "\n",
    "    return schedule\n",
    "\n",
    "\n",
    "model, base = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da6fee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_cb = MacroF1(val_ds)\n",
    "\n",
    "# Этап 1: учим только голову (быстро)\n",
    "total_steps1 = STEPS_PER_EPOCH * EPOCHS_STAGE1\n",
    "lr1 = tf.keras.optimizers.schedules.LearningRateSchedule(\n",
    "    cosine_lr(1e-3, 2e-4, total_steps1)\n",
    ")\n",
    "\n",
    "opt1 = tf.keras.optimizers.Adam(learning_rate=lr1)\n",
    "model.compile(\n",
    "    optimizer=opt1,\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(label_smoothing=0.1),\n",
    "    metrics=[\"sparse_categorical_accuracy\"],\n",
    ")\n",
    "\n",
    "cb1 = [\n",
    "    f1_cb,\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_macro_f1\",\n",
    "        mode=\"max\",\n",
    "        patience=2,\n",
    "        restore_best_weights=True,\n",
    "    ),\n",
    "]\n",
    "\n",
    "hist1 = model.fit(\n",
    "    get_train_ds(),\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    epochs=EPOCHS_STAGE1,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=cb1,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# Этап 2: разморозить верхнюю часть backbone\n",
    "base.trainable = True\n",
    "for layer in base.layers[:-60]:\n",
    "    layer.trainable = False\n",
    "\n",
    "total_steps2 = STEPS_PER_EPOCH * EPOCHS_STAGE2\n",
    "lr2 = tf.keras.optimizers.schedules.LearningRateSchedule(\n",
    "    cosine_lr(3e-5, 6e-6, total_steps2)\n",
    ")\n",
    "\n",
    "# AdamW чуть лучше держит fine-tune\n",
    "opt2 = tf.keras.optimizers.AdamW(learning_rate=lr2, weight_decay=1e-4)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=opt2,\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(label_smoothing=0.1),\n",
    "    metrics=[\"sparse_categorical_accuracy\"],\n",
    ")\n",
    "\n",
    "cb2 = [\n",
    "    f1_cb,\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_macro_f1\",\n",
    "        mode=\"max\",\n",
    "        patience=4,\n",
    "        restore_best_weights=True,\n",
    "    ),\n",
    "]\n",
    "\n",
    "hist2 = model.fit(\n",
    "    get_train_ds(),\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    epochs=EPOCHS_STAGE2,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=cb2,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# Этап 3: можно ещё чуть разморозить (если val_macro_f1 < 0.955)\n",
    "for layer in base.layers[:-20]:\n",
    "    layer.trainable = False\n",
    "\n",
    "total_steps3 = STEPS_PER_EPOCH * EPOCHS_STAGE3\n",
    "lr3 = tf.keras.optimizers.schedules.LearningRateSchedule(\n",
    "    cosine_lr(1e-5, 2e-6, total_steps3)\n",
    ")\n",
    "opt3 = tf.keras.optimizers.AdamW(learning_rate=lr3, weight_decay=1e-4)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=opt3,\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(label_smoothing=0.05),\n",
    "    metrics=[\"sparse_categorical_accuracy\"],\n",
    ")\n",
    "\n",
    "cb3 = [\n",
    "    f1_cb,\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_macro_f1\",\n",
    "        mode=\"max\",\n",
    "        patience=4,\n",
    "        restore_best_weights=True,\n",
    "    ),\n",
    "]\n",
    "\n",
    "hist3 = model.fit(\n",
    "    get_train_ds(),\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    epochs=EPOCHS_STAGE3,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=cb3,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf5ce85",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = get_test_ds()\n",
    "\n",
    "test_ids = []\n",
    "preds = []\n",
    "\n",
    "for x, ids in test_ds:\n",
    "    # TTA: обычный + горизонтальный флип\n",
    "    p1 = model.predict_on_batch(x)\n",
    "    p2 = model.predict_on_batch(tf.image.flip_left_right(x))\n",
    "    p = (p1 + p2) / 2.0\n",
    "\n",
    "    preds.append(np.argmax(p, axis=1))\n",
    "    test_ids.extend([i.decode(\"utf-8\") for i in ids.numpy()])\n",
    "\n",
    "preds = np.concatenate(preds)\n",
    "sub = pd.DataFrame({\"id\": test_ids, \"label\": preds.astype(int)})\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dca561",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3f7bde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916d09df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d1314d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0089746",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a5135f",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print(\"Running on TPU \", tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc478930",
   "metadata": {},
   "outputs": [],
   "source": [
    "GCS_DS_PATH = KaggleDatasets().get_gcs_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89d5a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = [\n",
    "    224,\n",
    "    224,\n",
    "]\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
    "NUM_CLASSES = 104\n",
    "\n",
    "GCS_PATH_SELECT = {  # available image sizes\n",
    "    192: GCS_DS_PATH + \"/tfrecords-jpeg-192x192\",\n",
    "    224: GCS_DS_PATH + \"/tfrecords-jpeg-224x224\",\n",
    "    331: GCS_DS_PATH + \"/tfrecords-jpeg-331x331\",\n",
    "    512: GCS_DS_PATH + \"/tfrecords-jpeg-512x512\",\n",
    "}\n",
    "GCS_PATH = GCS_PATH_SELECT[IMAGE_SIZE[0]]\n",
    "\n",
    "TRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH + \"/train/*.tfrec\")\n",
    "VALIDATION_FILENAMES = tf.io.gfile.glob(GCS_PATH + \"/val/*.tfrec\")\n",
    "TEST_FILENAMES = tf.io.gfile.glob(GCS_PATH + \"/test/*.tfrec\")\n",
    "\n",
    "AUTO = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0669603",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6f0632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Декодирование и обработка изображений\n",
    "\n",
    "\n",
    "def decode_and_resize(image_data):\n",
    "    \"\"\"\n",
    "    Декодирует JPEG-изображение и приводит его к заданному размеру.\n",
    "    Нормализует пиксели в диапазон [0, 1].\n",
    "    \"\"\"\n",
    "    image = tf.image.decode_jpeg(image_data, channels=3)\n",
    "    image = tf.image.resize(image, IMAGE_SIZE, method=\"bilinear\")\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image\n",
    "\n",
    "\n",
    "def augment_image(image, label):\n",
    "    \"\"\"\n",
    "    Применяет лёгкую аугментацию только к обучающим изображениям.\n",
    "    Помогает улучшить обобщающую способность модели.\n",
    "    \"\"\"\n",
    "    # Случайное горизонтальное отражение\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    # Случайные изменения яркости и насыщенности\n",
    "    image = tf.image.random_brightness(image, max_delta=0.15)\n",
    "    image = tf.image.random_saturation(image, lower=0.8, upper=1.2)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f263879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Чтение данных из TFRecord\n",
    "\n",
    "\n",
    "def read_labeled_record(example):\n",
    "    \"\"\"Парсит пример с меткой класса (для train/val).\"\"\"\n",
    "    features = tf.io.parse_single_example(\n",
    "        example,\n",
    "        {\n",
    "            \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "            \"class\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        },\n",
    "    )\n",
    "    image = decode_and_resize(features[\"image\"])\n",
    "    label = tf.cast(features[\"class\"], tf.int32)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "def read_unlabeled_record(example):\n",
    "    \"\"\"Парсит тестовый пример без метки — только id.\"\"\"\n",
    "    features = tf.io.parse_single_example(\n",
    "        example,\n",
    "        {\n",
    "            \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "            \"id\": tf.io.FixedLenFeature([], tf.string),\n",
    "        },\n",
    "    )\n",
    "    image = decode_and_resize(features[\"image\"])\n",
    "    return image, features[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38564fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Формирование датасетов\n",
    "\n",
    "\n",
    "def build_dataset(filenames, labeled=True, ordered=False, augment=False):\n",
    "    \"\"\"\n",
    "    Создаёт tf.data.Dataset из списка TFRecord-файлов.\n",
    "    Параметры:\n",
    "        labeled   — есть ли метки (train/val vs test)\n",
    "        ordered   — сохранять ли порядок (важно для сабмита)\n",
    "        augment   — применять ли аугментацию (только для train)\n",
    "    \"\"\"\n",
    "    options = tf.data.Options()\n",
    "    if not ordered:\n",
    "        options.experimental_deterministic = False  # ускоряет чтение\n",
    "\n",
    "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n",
    "    dataset = dataset.with_options(options)\n",
    "\n",
    "    parse_fn = read_labeled_record if labeled else read_unlabeled_record\n",
    "    dataset = dataset.map(parse_fn, num_parallel_calls=AUTO)\n",
    "\n",
    "    if augment:\n",
    "        dataset = dataset.map(augment_image, num_parallel_calls=AUTO)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_training_dataset():\n",
    "    \"\"\"Обучающий датасет с аугментацией, перемешиванием и повторением.\"\"\"\n",
    "    dataset = build_dataset(TRAINING_FILENAMES, labeled=True, augment=True)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.shuffle(buffer_size=2048)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTO)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_validation_dataset():\n",
    "    \"\"\"Валидационный датасет — без аугментации, с кэшированием.\"\"\"\n",
    "    dataset = build_dataset(VALIDATION_FILENAMES, labeled=True, ordered=False)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.cache()  # ускоряет валидацию при повторных эпохах\n",
    "    dataset = dataset.prefetch(AUTO)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_test_dataset(ordered=True):\n",
    "    \"\"\"Тестовый датасет — порядок важен для корректного сабмита.\"\"\"\n",
    "    dataset = build_dataset(TEST_FILENAMES, labeled=False, ordered=ordered)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTO)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc7c3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Вспомогательные функции\n",
    "\n",
    "\n",
    "def count_samples(filenames):\n",
    "    \"\"\"Подсчитывает общее число примеров по именам TFRecord-файлов.\"\"\"\n",
    "    counts = [int(re.search(r\"-([0-9]+)\\.\", f).group(1)) for f in filenames]\n",
    "    return np.sum(counts)\n",
    "\n",
    "\n",
    "# Расчёт количества примеров и шагов на эпоху\n",
    "NUM_TRAINING_IMAGES = count_samples(TRAINING_FILENAMES)\n",
    "NUM_VALIDATION_IMAGES = count_samples(VALIDATION_FILENAMES)\n",
    "NUM_TEST_IMAGES = count_samples(TEST_FILENAMES)\n",
    "STEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\n",
    "\n",
    "print(\n",
    "    f\"Загружено данных: \"\n",
    "    f\"{NUM_TRAINING_IMAGES} обучающих, \"\n",
    "    f\"{NUM_VALIDATION_IMAGES} валидационных, \"\n",
    "    f\"{NUM_TEST_IMAGES} тестовых изображений.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f69ebf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_block(x, filters, stride=1):\n",
    "    \"\"\"\n",
    "    Базовый ResNet-блок (как в ResNet-18/34).\n",
    "    \"\"\"\n",
    "    shortcut = x\n",
    "\n",
    "    # Основной путь\n",
    "    x = layers.Conv2D(\n",
    "        filters, 3, strides=stride, padding=\"same\", use_bias=False\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    x = layers.Conv2D(filters, 3, strides=1, padding=\"same\", use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    # Skip-соединение: согласуем размерность, если нужно\n",
    "    if stride != 1 or shortcut.shape[-1] != filters:\n",
    "        shortcut = layers.Conv2D(filters, 1, strides=stride, use_bias=False)(\n",
    "            shortcut\n",
    "        )\n",
    "        shortcut = layers.BatchNormalization()(shortcut)\n",
    "\n",
    "    # Сложение и активация\n",
    "    x = layers.Add()([x, shortcut])\n",
    "    x = layers.ReLU()(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def build_resnet34(input_shape=(331, 331, 3), num_classes=104):\n",
    "    \"\"\"\n",
    "    Создаёт ResNet-34-подобную сеть.\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    x = layers.Conv2D(64, 7, strides=2, padding=\"same\", use_bias=False)(\n",
    "        inputs\n",
    "    )  # 331 → 166\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.MaxPool2D(3, strides=2, padding=\"same\")(x)  # 166 → 83\n",
    "\n",
    "    # Стадии ResNet: [3, 4, 6, 3] блоков\n",
    "    # Stage 1: 64 filters, stride=1 → размер не меняется (83)\n",
    "    for _ in range(3):\n",
    "        x = basic_block(x, 64, stride=1)\n",
    "\n",
    "    # Stage 2: 128 filters, первый блок с stride=2 → 83 → 42\n",
    "    x = basic_block(x, 128, stride=2)\n",
    "    for _ in range(3):\n",
    "        x = basic_block(x, 128, stride=1)\n",
    "\n",
    "    # Stage 3: 256 filters, первый блок с stride=2 → 42 → 21\n",
    "    x = basic_block(x, 256, stride=2)\n",
    "    for _ in range(5):\n",
    "        x = basic_block(x, 256, stride=1)\n",
    "\n",
    "    # Stage 4: 512 filters, первый блок с stride=2 → 21 → 11\n",
    "    x = basic_block(x, 512, stride=2)\n",
    "    for _ in range(2):\n",
    "        x = basic_block(x, 512, stride=1)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    outputs = layers.Dense(\n",
    "        num_classes, activation=\"softmax\", name=\"predictions\"\n",
    "    )(x)\n",
    "\n",
    "    return Model(inputs, outputs, name=\"ResNet34_Custom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc731d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_resnet34(input_shape=(*IMAGE_SIZE, 3), num_classes=NUM_CLASSES)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Вывод информации о модели\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2ddcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    return build_resnet34(\n",
    "        input_shape=(*IMAGE_SIZE, 3), num_classes=NUM_CLASSES\n",
    "    )\n",
    "\n",
    "\n",
    "callbacks_list = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=6, restore_best_weights=True, verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.5, patience=3, verbose=1\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(\"Начинаем обучение модели...\")\n",
    "print(f\"  * Общее число эпох: {EPOCHS}\")\n",
    "print(\n",
    "    f\"  * Batch size: {BATCH_SIZE} (реплик: {strategy.num_replicas_in_sync})\"\n",
    ")\n",
    "print(f\"  * Шагов на эпоху: {STEPS_PER_EPOCH}\")\n",
    "print(f\"  * Обучающих изображений: {NUM_TRAINING_IMAGES}\")\n",
    "print(f\"  * Валидационных изображений: {NUM_VALIDATION_IMAGES}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# --- Создание и компиляция модели ---\n",
    "with strategy.scope():\n",
    "    model = get_model()\n",
    "    model.compile(\n",
    "        optimizer=\"nadam\",\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"sparse_categorical_accuracy\"],\n",
    "    )\n",
    "\n",
    "# --- Запуск обучения ---\n",
    "history = model.fit(\n",
    "    get_training_dataset(),\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=get_validation_dataset(),\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1,  # вывод по эпохам\n",
    ")\n",
    "\n",
    "print(\"Обучение завершено!\")\n",
    "print(f\"  * Использовано эпох: {len(history.history['loss'])}\")\n",
    "if \"lr\" in history.history:\n",
    "    print(f\"  * Финальная learning rate: {history.history['lr'][-1]:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd98375d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
