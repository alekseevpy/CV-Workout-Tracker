{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c32384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoModel, AutoProcessor\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "class GroundingDinoDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, processor, label_to_id):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.processor = processor\n",
    "        self.label_to_id = label_to_id\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        # Prepare text prompt (using label as description)\n",
    "        text = f\"a photo of {label}\"\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'text': text,\n",
    "            'label': self.label_to_id[label],\n",
    "            'image_path': image_path\n",
    "        }\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    \"\"\"Custom collate function to handle variable-sized images\"\"\"\n",
    "    images = [item['image'] for item in batch]\n",
    "    texts = [item['text'] for item in batch]\n",
    "    labels = torch.tensor([item['label'] for item in batch])\n",
    "    image_paths = [item['image_path'] for item in batch]\n",
    "    \n",
    "    return {\n",
    "        'images': images,\n",
    "        'texts': texts,\n",
    "        'labels': labels,\n",
    "        'image_paths': image_paths\n",
    "    }\n",
    "\n",
    "class GroundingDinoFineTuner:\n",
    "    def __init__(self, model_name=\"IDEA-Research/grounding-dino-base\"):\n",
    "        self.model_name = model_name\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.processor = None\n",
    "        self.model = None\n",
    "        self.label_to_id = {}\n",
    "        self.id_to_label = {}\n",
    "        \n",
    "    def setup_model(self):\n",
    "        \"\"\"Initialize the model and processor\"\"\"\n",
    "        self.processor = AutoProcessor.from_pretrained(self.model_name)\n",
    "        self.model = AutoModel.from_pretrained(self.model_name)\n",
    "        \n",
    "        # Freeze some layers to make training more stable\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Unfreeze the last few layers for fine-tuning\n",
    "        # Adjust based on your needs and computational resources\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if any(layer in name for layer in ['class_embed', 'bbox_embed', 'query_embed']):\n",
    "                param.requires_grad = True\n",
    "            if 'text_encoder' in name and 'layer.11' in name:  # Last layer of text encoder\n",
    "                param.requires_grad = True\n",
    "            if 'vision_model' in name and 'layer.23' in name:  # Last layer of vision model\n",
    "                param.requires_grad = True\n",
    "                \n",
    "        self.model.to(self.device)\n",
    "        print(f\"Model loaded on {self.device}\")\n",
    "        \n",
    "    def prepare_dataset(self, dataset_path, test_size=0.2, random_state=42):\n",
    "        \"\"\"Prepare dataset from folder structure\"\"\"\n",
    "        image_paths = []\n",
    "        labels = []\n",
    "        \n",
    "        # Get all label folders\n",
    "        label_folders = [f for f in os.listdir(dataset_path) \n",
    "                        if os.path.isdir(os.path.join(dataset_path, f))]\n",
    "        \n",
    "        # Create label mappings\n",
    "        self.label_to_id = {label: idx for idx, label in enumerate(label_folders)}\n",
    "        self.id_to_label = {idx: label for label, idx in self.label_to_id.items()}\n",
    "        \n",
    "        # Collect all image paths and labels\n",
    "        for label in label_folders:\n",
    "            label_path = os.path.join(dataset_path, label)\n",
    "            image_files = glob.glob(os.path.join(label_path, \"*.jpg\"))\n",
    "            \n",
    "            for img_path in image_files:\n",
    "                image_paths.append(img_path)\n",
    "                labels.append(label)\n",
    "        \n",
    "        print(f\"Found {len(image_paths)} images across {len(label_folders)} classes\")\n",
    "        print(f\"Classes: {list(self.label_to_id.keys())}\")\n",
    "        \n",
    "        # Split dataset\n",
    "        train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
    "            image_paths, labels, test_size=test_size, random_state=random_state, stratify=labels\n",
    "        )\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = GroundingDinoDataset(train_paths, train_labels, self.processor, self.label_to_id)\n",
    "        test_dataset = GroundingDinoDataset(test_paths, test_labels, self.processor, self.label_to_id)\n",
    "        \n",
    "        return train_dataset, test_dataset\n",
    "    \n",
    "    def process_batch(self, batch):\n",
    "        \"\"\"Process a batch of images and texts with proper settings for GroundingDino\"\"\"\n",
    "        try:\n",
    "            inputs = self.processor(\n",
    "                images=batch['images'], \n",
    "                text=batch['texts'], \n",
    "                return_tensors=\"pt\", \n",
    "                padding=True\n",
    "            )\n",
    "            return inputs\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch: {e}\")\n",
    "            # Fallback: process one by one\n",
    "            processed_inputs = []\n",
    "            for image, text in zip(batch['images'], batch['texts']):\n",
    "                try:\n",
    "                    input_single = self.processor(\n",
    "                        images=image, \n",
    "                        text=text, \n",
    "                        return_tensors=\"pt\"\n",
    "                    )\n",
    "                    processed_inputs.append(input_single)\n",
    "                except Exception as e2:\n",
    "                    print(f\"Error processing single item: {e2}\")\n",
    "                    continue\n",
    "            \n",
    "            # Manually collate\n",
    "            if processed_inputs:\n",
    "                keys = processed_inputs[0].keys()\n",
    "                batch_inputs = {}\n",
    "                for key in keys:\n",
    "                    batch_inputs[key] = torch.cat([inp[key] for inp in processed_inputs], dim=0)\n",
    "                return batch_inputs\n",
    "            else:\n",
    "                raise ValueError(\"Could not process any items in the batch\")\n",
    "    \n",
    "    def fine_tune(self, dataset_path, output_dir=\"./fine_tuned_weights\", \n",
    "                  batch_size=2, num_epochs=10, learning_rate=1e-5):\n",
    "        \"\"\"Fine-tune the GroundingDino model\"\"\"\n",
    "        \n",
    "        # Setup model\n",
    "        self.setup_model()\n",
    "        \n",
    "        # Prepare datasets\n",
    "        train_dataset, test_dataset = self.prepare_dataset(dataset_path)\n",
    "        \n",
    "        if len(train_dataset) == 0:\n",
    "            raise ValueError(\"No training data found!\")\n",
    "        \n",
    "        # Create data loaders with custom collate function\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=True,\n",
    "            collate_fn=custom_collate_fn,\n",
    "            num_workers=0\n",
    "        )\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=False,\n",
    "            collate_fn=custom_collate_fn,\n",
    "            num_workers=0\n",
    "        )\n",
    "        \n",
    "        # Optimizer - only trainable parameters\n",
    "        trainable_params = [p for p in self.model.parameters() if p.requires_grad]\n",
    "        optimizer = AdamW(trainable_params, lr=learning_rate, weight_decay=1e-4)\n",
    "        \n",
    "        # Training loop\n",
    "        self.model.train()\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0\n",
    "            progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            \n",
    "            for batch_idx, batch in enumerate(progress_bar):\n",
    "                try:\n",
    "                    # Process batch\n",
    "                    inputs = self.process_batch(batch)\n",
    "                    inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "                    labels = batch['labels'].to(self.device)\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    outputs = self.model(**inputs)\n",
    "                    \n",
    "                    # Compute loss - adapted for GroundingDino\n",
    "                    loss = self.compute_detection_loss(outputs, labels, batch['texts'])\n",
    "                    \n",
    "                    # Backward pass\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(trainable_params, max_norm=1.0)\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    total_loss += loss.item()\n",
    "                    progress_bar.set_postfix({\"Loss\": loss.item()})\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error in batch {batch_idx}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            if len(train_loader) > 0:\n",
    "                avg_loss = total_loss / len(train_loader)\n",
    "                print(f\"Epoch {epoch+1}, Average Loss: {avg_loss:.4f}\")\n",
    "                \n",
    "                # Validate\n",
    "                self.validate(test_loader)\n",
    "        \n",
    "        # Save model and metadata\n",
    "        self.save_model(output_dir)\n",
    "        \n",
    "        return output_dir\n",
    "    \n",
    "    def compute_detection_loss(self, outputs, labels, texts):\n",
    "        \"\"\"\n",
    "        Compute loss for GroundingDino - this is a simplified approach\n",
    "        Since GroundingDino is a detection model, we need to adapt the loss\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # GroundingDino outputs contain predictions and logits\n",
    "            # We'll use a simple approach: treat it as classification on the best box\n",
    "            \n",
    "            if hasattr(outputs, 'logits') and outputs.logits is not None:\n",
    "                # Get the classification logits for the best detection\n",
    "                batch_size = outputs.logits.shape[0]\n",
    "                total_loss = 0\n",
    "                \n",
    "                for i in range(batch_size):\n",
    "                    # Get the detection with highest confidence\n",
    "                    max_conf_idx = torch.argmax(outputs.logits[i].max(dim=1).values)\n",
    "                    \n",
    "                    # Use cross entropy loss on the predicted class\n",
    "                    # This is a simplification - you might need to adapt this based on your needs\n",
    "                    if max_conf_idx < outputs.logits[i].shape[0]:\n",
    "                        class_logits = outputs.logits[i][max_conf_idx]\n",
    "                        \n",
    "                        # Simple classification loss\n",
    "                        # Note: This assumes your labels match the text queries\n",
    "                        loss_fn = nn.CrossEntropyLoss()\n",
    "                        loss = loss_fn(class_logits.unsqueeze(0), labels[i].unsqueeze(0))\n",
    "                        total_loss += loss\n",
    "                \n",
    "                return total_loss / batch_size if batch_size > 0 else torch.tensor(0.0)\n",
    "            else:\n",
    "                # Fallback: simple classification loss using hidden states\n",
    "                return self.compute_classification_loss(outputs, labels)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in loss computation: {e}\")\n",
    "            return self.compute_classification_loss(outputs, labels)\n",
    "    \n",
    "    def compute_classification_loss(self, outputs, labels):\n",
    "        \"\"\"Fallback classification loss using hidden states\"\"\"\n",
    "        try:\n",
    "            if hasattr(outputs, 'last_hidden_state'):\n",
    "                # Use average pooling over sequence dimension\n",
    "                pooled_output = outputs.last_hidden_state.mean(dim=1)\n",
    "                classifier = nn.Linear(pooled_output.size(-1), len(self.label_to_id)).to(self.device)\n",
    "                logits = classifier(pooled_output)\n",
    "                loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "                return loss\n",
    "            else:\n",
    "                return torch.tensor(0.0, requires_grad=True).to(self.device)\n",
    "        except:\n",
    "            return torch.tensor(0.0, requires_grad=True).to(self.device)\n",
    "    \n",
    "    def validate(self, test_loader):\n",
    "        \"\"\"Simple validation function\"\"\"\n",
    "        self.model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                try:\n",
    "                    inputs = self.process_batch(batch)\n",
    "                    inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "                    labels = batch['labels'].to(self.device)\n",
    "                    \n",
    "                    outputs = self.model(**inputs)\n",
    "                    \n",
    "                    # Simple accuracy calculation\n",
    "                    if hasattr(outputs, 'last_hidden_state'):\n",
    "                        pooled_output = outputs.last_hidden_state.mean(dim=1)\n",
    "                        classifier = nn.Linear(pooled_output.size(-1), len(self.label_to_id)).to(self.device)\n",
    "                        logits = classifier(pooled_output)\n",
    "                        predictions = torch.argmax(logits, dim=1)\n",
    "                        correct += (predictions == labels).sum().item()\n",
    "                        total += labels.size(0)\n",
    "                except Exception as e:\n",
    "                    print(f\"Validation error: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        if total > 0:\n",
    "            accuracy = correct / total\n",
    "            print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "        else:\n",
    "            print(\"No validation samples processed\")\n",
    "            \n",
    "        self.model.train()\n",
    "    \n",
    "    def save_model(self, output_dir):\n",
    "        \"\"\"Save fine-tuned model and metadata\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Save model weights\n",
    "        torch.save(self.model.state_dict(), os.path.join(output_dir, \"pytorch_model.bin\"))\n",
    "        \n",
    "        # Save metadata\n",
    "        metadata = {\n",
    "            \"label_to_id\": self.label_to_id,\n",
    "            \"id_to_label\": self.id_to_label,\n",
    "            \"model_name\": self.model_name\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(output_dir, \"metadata.json\"), \"w\") as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        # Save processor\n",
    "        self.processor.save_pretrained(output_dir)\n",
    "        \n",
    "        print(f\"Model saved to {output_dir}\")\n",
    "\n",
    "class QuickGroundingDino:\n",
    "    def __init__(self, weights_path):\n",
    "        \"\"\"Quick loader for fine-tuned weights\"\"\"\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.weights_path = weights_path\n",
    "        self.model = None\n",
    "        self.processor = None\n",
    "        self.label_to_id = {}\n",
    "        self.id_to_label = {}\n",
    "        \n",
    "        self.load_model()\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"Load fine-tuned model and metadata\"\"\"\n",
    "        # Load metadata\n",
    "        with open(os.path.join(self.weights_path, \"metadata.json\"), \"r\") as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        self.label_to_id = metadata[\"label_to_id\"]\n",
    "        self.id_to_label = {int(k): v for k, v in metadata[\"id_to_label\"].items()}\n",
    "        model_name = metadata[\"model_name\"]\n",
    "        \n",
    "        # Load processor and model\n",
    "        self.processor = AutoProcessor.from_pretrained(self.weights_path)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Load fine-tuned weights\n",
    "        model_path = os.path.join(self.weights_path, \"pytorch_model.bin\")\n",
    "        self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        print(f\"Model loaded from {self.weights_path}\")\n",
    "        print(f\"Available classes: {list(self.label_to_id.keys())}\")\n",
    "    \n",
    "    def predict(self, image_path):\n",
    "        \"\"\"Make prediction on a single image\"\"\"\n",
    "        try:\n",
    "            # Load image\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            \n",
    "            # Try all possible labels and find the best match\n",
    "            best_label = \"unknown\"\n",
    "            best_confidence = 0.0\n",
    "            all_predictions = {}\n",
    "            \n",
    "            for label in self.label_to_id.keys():\n",
    "                text = f\"a photo of {label}\"\n",
    "                \n",
    "                try:\n",
    "                    # Process inputs\n",
    "                    inputs = self.processor(images=image, text=text, return_tensors=\"pt\")\n",
    "                    inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "                    \n",
    "                    # Get model output\n",
    "                    with torch.no_grad():\n",
    "                        outputs = self.model(**inputs)\n",
    "                    \n",
    "                    # Calculate confidence score\n",
    "                    confidence = self.calculate_confidence(outputs, label)\n",
    "                    all_predictions[label] = confidence\n",
    "                    \n",
    "                    if confidence > best_confidence:\n",
    "                        best_confidence = confidence\n",
    "                        best_label = label\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing label {label}: {e}\")\n",
    "                    all_predictions[label] = 0.0\n",
    "            \n",
    "            return {\n",
    "                \"predicted_label\": best_label,\n",
    "                \"confidence\": best_confidence,\n",
    "                \"all_predictions\": all_predictions\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in prediction: {e}\")\n",
    "            return {\n",
    "                \"predicted_label\": \"error\",\n",
    "                \"confidence\": 0.0,\n",
    "                \"all_predictions\": {}\n",
    "            }\n",
    "    \n",
    "    def calculate_confidence(self, outputs, label):\n",
    "        \"\"\"Calculate confidence score from model outputs\"\"\"\n",
    "        try:\n",
    "            # Method 1: Use detection confidence if available\n",
    "            if hasattr(outputs, 'logits') and outputs.logits is not None:\n",
    "                # Get maximum detection confidence\n",
    "                max_confidence = torch.sigmoid(outputs.logits).max().item()\n",
    "                return max_confidence\n",
    "            \n",
    "            # Method 2: Use hidden states for classification\n",
    "            elif hasattr(outputs, 'last_hidden_state'):\n",
    "                pooled_output = outputs.last_hidden_state.mean(dim=1)\n",
    "                classifier = nn.Linear(pooled_output.size(-1), len(self.label_to_id)).to(self.device)\n",
    "                logits = classifier(pooled_output)\n",
    "                probabilities = torch.softmax(logits, dim=1)\n",
    "                return probabilities[0, self.label_to_id[label]].item()\n",
    "            \n",
    "            else:\n",
    "                return 0.5  # Default confidence\n",
    "                \n",
    "        except:\n",
    "            return 0.0\n",
    "\n",
    "def train_grounding_dino_safe():\n",
    "    \"\"\"Safe training function with error handling\"\"\"\n",
    "    try:\n",
    "        fine_tuner = GroundingDinoFineTuner()\n",
    "        \n",
    "        output_dir = fine_tuner.fine_tune(\n",
    "            dataset_path=\"fine_tuning_dataset\",\n",
    "            output_dir=\"./fine_tuned_grounding_dino\",\n",
    "            batch_size=1,  # Start with batch size 1 for stability\n",
    "            num_epochs=3,  # Start with fewer epochs\n",
    "            learning_rate=1e-5\n",
    "        )\n",
    "        \n",
    "        return output_dir\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Training failed: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b30728a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Model loaded on cuda\n",
      "Found 611 images across 1 classes\n",
      "Classes: ['pull up']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   1%|          | 3/488 [01:58<5:18:50, 39.45s/it, Loss=0]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m weights_path = \u001b[43mtrain_grounding_dino_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weights_path:\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining completed! Weights saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweights_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 459\u001b[39m, in \u001b[36mtrain_grounding_dino_safe\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    456\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    457\u001b[39m     fine_tuner = GroundingDinoFineTuner()\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m     output_dir = \u001b[43mfine_tuner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfine_tune\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfine_tuning_dataset\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./fine_tuned_grounding_dino\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Start with batch size 1 for stability\u001b[39;49;00m\n\u001b[32m    463\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Start with fewer epochs\u001b[39;49;00m\n\u001b[32m    464\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-5\u001b[39;49m\n\u001b[32m    465\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    467\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output_dir\n\u001b[32m    469\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 206\u001b[39m, in \u001b[36mGroundingDinoFineTuner.fine_tune\u001b[39m\u001b[34m(self, dataset_path, output_dir, batch_size, num_epochs, learning_rate)\u001b[39m\n\u001b[32m    203\u001b[39m labels = batch[\u001b[33m'\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m'\u001b[39m].to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m    205\u001b[39m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[38;5;66;03m# Compute loss - adapted for GroundingDino\u001b[39;00m\n\u001b[32m    209\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.compute_detection_loss(outputs, labels, batch[\u001b[33m'\u001b[39m\u001b[33mtexts\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Study\\Masters\\term_3\\CV-Workout-Tracker\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Study\\Masters\\term_3\\CV-Workout-Tracker\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Study\\Masters\\term_3\\CV-Workout-Tracker\\.venv\\Lib\\site-packages\\transformers\\models\\grounding_dino\\modeling_grounding_dino.py:2195\u001b[39m, in \u001b[36mGroundingDinoModel.forward\u001b[39m\u001b[34m(self, pixel_values, input_ids, token_type_ids, attention_mask, pixel_mask, encoder_outputs, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   2192\u001b[39m \u001b[38;5;66;03m# Fourth, sent source_flatten + mask_flatten + lvl_pos_embed_flatten (backbone + proj layer output) through encoder\u001b[39;00m\n\u001b[32m   2193\u001b[39m \u001b[38;5;66;03m# Also provide spatial_shapes, level_start_index and valid_ratios\u001b[39;00m\n\u001b[32m   2194\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m encoder_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2195\u001b[39m     encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2196\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvision_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43msource_flatten\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2197\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvision_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43m~\u001b[49m\u001b[43mmask_flatten\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2198\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvision_position_embedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlvl_pos_embed_flatten\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2199\u001b[39m \u001b[43m        \u001b[49m\u001b[43mspatial_shapes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspatial_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2200\u001b[39m \u001b[43m        \u001b[49m\u001b[43mspatial_shapes_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspatial_shapes_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2201\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevel_start_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel_start_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2202\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalid_ratios\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalid_ratios\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2203\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2204\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43m~\u001b[49m\u001b[43mtext_token_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2205\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_position_embedding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2206\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_self_attention_masks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m~\u001b[49m\u001b[43mtext_self_attention_masks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2207\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_position_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2208\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2209\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2210\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2211\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2212\u001b[39m \u001b[38;5;66;03m# If the user passed a tuple for encoder_outputs, we wrap it in a GroundingDinoEncoderOutput when return_dict=True\u001b[39;00m\n\u001b[32m   2213\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoder_outputs, GroundingDinoEncoderOutput):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Study\\Masters\\term_3\\CV-Workout-Tracker\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Study\\Masters\\term_3\\CV-Workout-Tracker\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Study\\Masters\\term_3\\CV-Workout-Tracker\\.venv\\Lib\\site-packages\\transformers\\models\\grounding_dino\\modeling_grounding_dino.py:1584\u001b[39m, in \u001b[36mGroundingDinoEncoder.forward\u001b[39m\u001b[34m(self, vision_features, vision_attention_mask, vision_position_embedding, spatial_shapes, spatial_shapes_list, level_start_index, valid_ratios, text_features, text_attention_mask, text_position_embedding, text_self_attention_masks, text_position_ids, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1581\u001b[39m     encoder_vision_states += (vision_features,)\n\u001b[32m   1582\u001b[39m     encoder_text_states += (text_features,)\n\u001b[32m-> \u001b[39m\u001b[32m1584\u001b[39m (vision_features, text_features), attentions = \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1585\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvision_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvision_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1586\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvision_position_embedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvision_position_embedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1587\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspatial_shapes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspatial_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1588\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspatial_shapes_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspatial_shapes_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1589\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlevel_start_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel_start_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1590\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvision_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1591\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreference_points\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreference_points\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1592\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1593\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1594\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_position_embedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_position_embedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1595\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_self_attention_masks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_self_attention_masks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1596\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_position_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_position_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1597\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1599\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[32m   1600\u001b[39m     all_attn_fused_vision += (attentions[\u001b[32m0\u001b[39m],)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Study\\Masters\\term_3\\CV-Workout-Tracker\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Study\\Masters\\term_3\\CV-Workout-Tracker\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Study\\Masters\\term_3\\CV-Workout-Tracker\\.venv\\Lib\\site-packages\\transformers\\models\\grounding_dino\\modeling_grounding_dino.py:1147\u001b[39m, in \u001b[36mGroundingDinoEncoderLayer.forward\u001b[39m\u001b[34m(self, vision_features, vision_position_embedding, spatial_shapes, spatial_shapes_list, level_start_index, key_padding_mask, reference_points, text_features, text_attention_mask, text_position_embedding, text_self_attention_masks, text_position_ids)\u001b[39m\n\u001b[32m   1134\u001b[39m (vision_features, vision_fused_attn), (text_features, text_fused_attn) = \u001b[38;5;28mself\u001b[39m.fusion_layer(\n\u001b[32m   1135\u001b[39m     vision_features=vision_features,\n\u001b[32m   1136\u001b[39m     text_features=text_features,\n\u001b[32m   1137\u001b[39m     attention_mask_vision=key_padding_mask,\n\u001b[32m   1138\u001b[39m     attention_mask_text=text_attention_mask,\n\u001b[32m   1139\u001b[39m )\n\u001b[32m   1141\u001b[39m (text_features, text_enhanced_attn) = \u001b[38;5;28mself\u001b[39m.text_enhancer_layer(\n\u001b[32m   1142\u001b[39m     hidden_states=text_features,\n\u001b[32m   1143\u001b[39m     attention_masks=~text_self_attention_masks,  \u001b[38;5;66;03m# note we use ~ for mask here\u001b[39;00m\n\u001b[32m   1144\u001b[39m     position_embeddings=(text_position_embedding \u001b[38;5;28;01mif\u001b[39;00m text_position_embedding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m   1145\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1147\u001b[39m (vision_features, vision_deformable_attn) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdeformable_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvision_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43m~\u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvision_position_embedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreference_points\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreference_points\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspatial_shapes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspatial_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspatial_shapes_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspatial_shapes_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1154\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlevel_start_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel_start_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1155\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1157\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m   1158\u001b[39m     (vision_features, text_features),\n\u001b[32m   1159\u001b[39m     (vision_fused_attn, text_fused_attn, text_enhanced_attn, vision_deformable_attn),\n\u001b[32m   1160\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Study\\Masters\\term_3\\CV-Workout-Tracker\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Study\\Masters\\term_3\\CV-Workout-Tracker\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Study\\Masters\\term_3\\CV-Workout-Tracker\\.venv\\Lib\\site-packages\\transformers\\models\\grounding_dino\\modeling_grounding_dino.py:1039\u001b[39m, in \u001b[36mGroundingDinoDeformableLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_embeddings, reference_points, spatial_shapes, spatial_shapes_list, level_start_index, output_attentions)\u001b[39m\n\u001b[32m   1036\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.final_layer_norm(hidden_states)\n\u001b[32m   1038\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training:\n\u001b[32m-> \u001b[39m\u001b[32m1039\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch.isinf(hidden_states).any() \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43misnan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m.any():\n\u001b[32m   1040\u001b[39m         clamp_value = torch.finfo(hidden_states.dtype).max - \u001b[32m1000\u001b[39m\n\u001b[32m   1041\u001b[39m         hidden_states = torch.clamp(hidden_states, \u001b[38;5;28mmin\u001b[39m=-clamp_value, \u001b[38;5;28mmax\u001b[39m=clamp_value)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "weights_path = train_grounding_dino_safe()\n",
    "if weights_path:\n",
    "    print(f\"Training completed! Weights saved to: {weights_path}\")\n",
    "else:\n",
    "    print(\"Training failed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5ef54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_grounding_dino(weights_path, test_image_path):\n",
    "    \"\"\"Quick test function\"\"\"\n",
    "    if not os.path.exists(weights_path):\n",
    "        print(f\"Weights path {weights_path} does not exist!\")\n",
    "        return\n",
    "    \n",
    "    if not os.path.exists(test_image_path):\n",
    "        print(f\"Test image {test_image_path} does not exist!\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        model = QuickGroundingDino(weights_path)\n",
    "        result = model.predict(test_image_path)\n",
    "        \n",
    "        print(f\"\\n=== Prediction Results ===\")\n",
    "        print(f\"Image: {os.path.basename(test_image_path)}\")\n",
    "        print(f\"Predicted Label: {result['predicted_label']}\")\n",
    "        print(f\"Confidence: {result['confidence']:.4f}\")\n",
    "        print(f\"\\nAll predictions:\")\n",
    "        for label, score in sorted(result['all_predictions'].items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"  {label}: {score:.4f}\")\n",
    "        \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Testing failed: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19aad445",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path = ''\n",
    "test_image_path = ''\n",
    "test_grounding_dino(weights_path, test_image_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
