{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8d4f234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0bb06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorkoutImageDataset(Dataset):\n",
    "    def __init__(self, data_dir, annotations_file, processor, transform=None, max_samples=None):\n",
    "        self.processor = processor\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Load annotations\n",
    "        if annotations_file.endswith('.csv'):\n",
    "            self.annotations = pd.read_csv(annotations_file)\n",
    "        else:  # JSON format\n",
    "            with open(annotations_file, 'r') as f:\n",
    "                self.annotations = json.load(f)\n",
    "        \n",
    "        # Prepare samples\n",
    "        self.samples = self.prepare_samples()\n",
    "        \n",
    "        if max_samples:\n",
    "            self.samples = self.samples[:max_samples]\n",
    "    \n",
    "    def prepare_samples(self):\n",
    "        \"\"\"Prepare image-annotation pairs\"\"\"\n",
    "        samples = []\n",
    "        \n",
    "        if isinstance(self.annotations, pd.DataFrame):\n",
    "            # CSV format\n",
    "            for _, row in self.annotations.iterrows():\n",
    "                image_path = self.data_dir / row['image_path']\n",
    "                if image_path.exists():\n",
    "                    samples.append({\n",
    "                        'image_path': str(image_path),\n",
    "                        'bboxes': eval(row['bboxes']) if isinstance(row['bboxes'], str) else row['bboxes'],\n",
    "                        'labels': eval(row['labels']) if isinstance(row['labels'], str) else row['labels']\n",
    "                    })\n",
    "        else:\n",
    "            # JSON format\n",
    "            for image_name, image_info in self.annotations.items():\n",
    "                image_path = self.data_dir / image_name\n",
    "                if image_path.exists():\n",
    "                    samples.append({\n",
    "                        'image_path': str(image_path),\n",
    "                        'bboxes': image_info.get('bboxes', []),\n",
    "                        'labels': image_info.get('labels', [])\n",
    "                    })\n",
    "        \n",
    "        print(f\"Loaded {len(samples)} samples\")\n",
    "        return samples\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(sample['image_path']).convert('RGB')\n",
    "        \n",
    "        # Apply transforms if any\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Get bounding boxes and labels\n",
    "        bboxes = sample['bboxes']\n",
    "        labels = sample['labels']\n",
    "        \n",
    "        # Convert bboxes to tensor\n",
    "        if bboxes:\n",
    "            # Ensure bboxes are in [x1, y1, x2, y2] format normalized [0,1]\n",
    "            bboxes_tensor = torch.tensor(bboxes, dtype=torch.float)\n",
    "        else:\n",
    "            bboxes_tensor = torch.zeros((0, 4), dtype=torch.float)\n",
    "        \n",
    "        # Create text prompt\n",
    "        text_prompt = \" . \".join(labels) if labels else \"exercise\"\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'text_prompt': text_prompt,\n",
    "            'boxes': bboxes_tensor,\n",
    "            'labels': labels,\n",
    "            'image_path': sample['image_path']\n",
    "        }\n",
    "\n",
    "# Data augmentation transforms\n",
    "def get_transforms(train=True):\n",
    "    if train:\n",
    "        return torch.nn.Sequential(\n",
    "            # You can add more transforms here\n",
    "            # torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "            # torchvision.transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        )\n",
    "    return torch.nn.Sequential()  # Identity transform for validation\n",
    "\n",
    "# Custom collate function\n",
    "def collate_fn(batch):\n",
    "    images = [item['image'] for item in batch]\n",
    "    text_prompts = [item['text_prompt'] for item in batch]\n",
    "    boxes = [item['boxes'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "    image_paths = [item['image_path'] for item in batch]\n",
    "    \n",
    "    return {\n",
    "        'images': images,\n",
    "        'text_prompts': text_prompts,\n",
    "        'boxes': boxes,\n",
    "        'labels': labels,\n",
    "        'image_paths': image_paths\n",
    "    }\n",
    "\n",
    "# Loss function for Grounding DINO\n",
    "class GroundingDINOLoss(nn.Module):\n",
    "    def __init__(self, weight_dict=None):\n",
    "        super().__init__()\n",
    "        self.weight_dict = weight_dict or {\n",
    "            'loss_ce': 1.0,\n",
    "            'loss_bbox': 1.0,\n",
    "            'loss_giou': 1.0,\n",
    "        }\n",
    "    \n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\"\n",
    "        Compute losses\n",
    "        outputs: model outputs\n",
    "        targets: list of target boxes\n",
    "        \"\"\"\n",
    "        losses = {}\n",
    "        \n",
    "        # Classification loss (simplified)\n",
    "        if hasattr(outputs, 'logits'):\n",
    "            # You might need to adapt this based on actual model output\n",
    "            pass\n",
    "        \n",
    "        # Box regression losses\n",
    "        total_boxes = sum(len(target) for target in targets)\n",
    "        if total_boxes > 0:\n",
    "            # Simplified box loss - you'll need to adapt this\n",
    "            loss_bbox = torch.tensor(0.0, device=outputs.last_hidden_state.device)\n",
    "            loss_giou = torch.tensor(0.0, device=outputs.last_hidden_state.device)\n",
    "            \n",
    "            # This is a placeholder - you'll need to implement proper matching\n",
    "            for i, (output, target) in enumerate(zip(outputs.pred_boxes, targets)):\n",
    "                if len(target) > 0:\n",
    "                    # Simple L1 loss for boxes (normalized coordinates)\n",
    "                    pred_boxes = output[:len(target)]\n",
    "                    loss_bbox += torch.nn.functional.l1_loss(pred_boxes, target)\n",
    "            \n",
    "            losses['loss_bbox'] = loss_bbox / len(targets)\n",
    "            losses['loss_giou'] = loss_giou / len(targets)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = sum(losses.get(k, 0) * self.weight_dict.get(k, 1.0) for k in losses)\n",
    "        losses['total_loss'] = total_loss\n",
    "        \n",
    "        return losses\n",
    "\n",
    "# Training function\n",
    "def train_grounding_dino(model, processor, train_loader, val_loader, num_epochs=10, device='cuda', output_dir='checkpoints'):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=1e-4,\n",
    "        weight_decay=1e-4\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = GroundingDINOLoss()\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_items = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            images = batch['images']\n",
    "            text_prompts = batch['text_prompts']\n",
    "            target_boxes = batch['boxes']\n",
    "            \n",
    "            # Process inputs\n",
    "            inputs = processor(\n",
    "                images=images,\n",
    "                text=text_prompts,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True\n",
    "            ).to(device)\n",
    "            \n",
    "            # Move target boxes to device\n",
    "            target_boxes = [boxes.to(device) for boxes in target_boxes]\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            # Compute loss\n",
    "            losses = criterion(outputs, target_boxes)\n",
    "            loss = losses['total_loss']\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_items += len(images)\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'avg_loss': f'{total_loss/total_items:.4f}'\n",
    "            })\n",
    "        \n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch+1}, Average Train Loss: {avg_train_loss:.4f}')\n",
    "        \n",
    "        # Validation\n",
    "        if val_loader:\n",
    "            val_loss = validate_model(model, processor, val_loader, criterion, device)\n",
    "            print(f'Epoch {epoch+1}, Validation Loss: {val_loss:.4f}')\n",
    "            \n",
    "            # Save best model\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(model.state_dict(), f'{output_dir}/best_model.pth')\n",
    "                print(f\"New best model saved with val_loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'loss': avg_train_loss,\n",
    "        }, f'{output_dir}/checkpoint_epoch_{epoch+1}.pth')\n",
    "        \n",
    "        scheduler.step()\n",
    "    \n",
    "    print(\"Training completed!\")\n",
    "\n",
    "def validate_model(model, processor, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc='Validating'):\n",
    "            images = batch['images']\n",
    "            text_prompts = batch['text_prompts']\n",
    "            target_boxes = batch['boxes']\n",
    "            \n",
    "            inputs = processor(\n",
    "                images=images,\n",
    "                text=text_prompts,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True\n",
    "            ).to(device)\n",
    "            \n",
    "            target_boxes = [boxes.to(device) for boxes in target_boxes]\n",
    "            \n",
    "            outputs = model(**inputs)\n",
    "            losses = criterion(outputs, target_boxes)\n",
    "            total_loss += losses['total_loss'].item()\n",
    "    \n",
    "    avg_val_loss = total_loss / len(val_loader)\n",
    "    return avg_val_loss\n",
    "\n",
    "# Main training script\n",
    "def main():\n",
    "    # Initialize model and processor\n",
    "    model_name = \"IDEA-Research/grounding-dino-base\"\n",
    "    processor = AutoProcessor.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    \n",
    "    # Dataset paths - adjust based on your downloaded dataset structure\n",
    "    data_dir = \"workoutexercises-images\"  # Path to extracted dataset\n",
    "    annotations_file = \"workoutexercises-images/annotations.json\"  # or annotations.csv\n",
    "    \n",
    "    # Create transforms\n",
    "    train_transform = get_transforms(train=True)\n",
    "    val_transform = get_transforms(train=False)\n",
    "    \n",
    "    # Create datasets\n",
    "    dataset = WorkoutImageDataset(\n",
    "        data_dir=data_dir,\n",
    "        annotations_file=annotations_file,\n",
    "        processor=processor,\n",
    "        transform=train_transform,\n",
    "        max_samples=1000  # Limit for testing, remove for full training\n",
    "    )\n",
    "    \n",
    "    # Split into train/val\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    # Update transforms for validation\n",
    "    val_dataset.dataset.transform = val_transform\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    \n",
    "    # Train model\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    train_grounding_dino(\n",
    "        model=model,\n",
    "        processor=processor,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        num_epochs=20,\n",
    "        device=device,\n",
    "        output_dir='workout_finetuned_models'\n",
    "    )\n",
    "\n",
    "# Utility function to create annotations if they don't exist\n",
    "def create_dummy_annotations(data_dir, output_file):\n",
    "    \"\"\"Create dummy annotations for testing - you'll need real annotations\"\"\"\n",
    "    annotations = {}\n",
    "    \n",
    "    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp']\n",
    "    image_files = []\n",
    "    \n",
    "    for ext in image_extensions:\n",
    "        image_files.extend(Path(data_dir).glob(f'**/*{ext}'))\n",
    "    \n",
    "    for image_path in image_files:\n",
    "        # Create dummy annotations - replace with real ones\n",
    "        annotations[image_path.name] = {\n",
    "            'bboxes': [[0.1, 0.1, 0.3, 0.3]],  # [x1, y1, x2, y2] normalized\n",
    "            'labels': ['dumbbell']  # Replace with actual exercise labels\n",
    "        }\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(annotations, f, indent=2)\n",
    "    \n",
    "    print(f\"Created dummy annotations for {len(annotations)} images at {output_file}\")\n",
    "\n",
    "# Inference with fine-tuned model\n",
    "def inference_finetuned(image_path, candidate_labels, model_path, box_threshold=0.3):\n",
    "    \"\"\"Run inference with fine-tuned model\"\"\"\n",
    "    processor = AutoProcessor.from_pretrained(\"IDEA-Research/grounding-dino-base\")\n",
    "    model = AutoModel.from_pretrained(\"IDEA-Research/grounding-dino-base\")\n",
    "    \n",
    "    # Load fine-tuned weights\n",
    "    model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "    model.eval()\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    \n",
    "    # Load and process image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    text_prompt = \" . \".join(candidate_labels)\n",
    "    \n",
    "    inputs = processor(images=image, text=text_prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Process outputs (you'll need to adapt this based on your training)\n",
    "    # This is a simplified version - you might need custom post-processing\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # First, download the dataset\n",
    "    print(\"Please download the dataset from Kaggle first:\")\n",
    "    print(\"kaggle datasets download -d hasyimabdillah/workoutexercises-images\")\n",
    "    \n",
    "    # Extract if needed\n",
    "    import zipfile\n",
    "    zip_path = \"workoutexercises-images.zip\"\n",
    "    if os.path.exists(zip_path):\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(\"workoutexercises-images\")\n",
    "        print(\"Dataset extracted!\")\n",
    "    \n",
    "    # Check if annotations exist, create dummy ones if not\n",
    "    annotations_path = \"workoutexercises-images/annotations.json\"\n",
    "    if not os.path.exists(annotations_path):\n",
    "        print(\"Creating dummy annotations for testing...\")\n",
    "        create_dummy_annotations(\"workoutexercises-images\", annotations_path)\n",
    "        print(\"Please replace dummy annotations with real ones!\")\n",
    "    \n",
    "    # Start training\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6052ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Custom collate function\n",
    "def collate_fn(batch):\n",
    "    images = [item['image'] for item in batch]\n",
    "    text_prompts = [item['text_prompt'] for item in batch]\n",
    "    boxes = [item['boxes'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "    \n",
    "    return {\n",
    "        'images': images,\n",
    "        'text_prompts': text_prompts,\n",
    "        'boxes': boxes,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "# Training function\n",
    "def train_grounding_dino(model, processor, train_loader, val_loader, num_epochs=10, device='cuda'):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "    \n",
    "    # Loss function\n",
    "    def compute_loss(outputs, targets):\n",
    "        # Simplified loss function - you might need to adapt this based on Grounding DINO's output\n",
    "        loss_dict = outputs.loss_dict\n",
    "        total_loss = sum(loss_dict.values())\n",
    "        return total_loss\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            images = batch['images']\n",
    "            text_prompts = batch['text_prompts']\n",
    "            target_boxes = batch['boxes']\n",
    "            \n",
    "            # Process inputs\n",
    "            inputs = processor(\n",
    "                images=images,\n",
    "                text=text_prompts,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True\n",
    "            ).to(device)\n",
    "            \n",
    "            # Move target boxes to device\n",
    "            target_boxes = [boxes.to(device) for boxes in target_boxes]\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = compute_loss(outputs, target_boxes)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch+1}, Average Loss: {avg_loss:.4f}')\n",
    "        \n",
    "        # Validation\n",
    "        if val_loader:\n",
    "            validate_model(model, processor, val_loader, device)\n",
    "        \n",
    "        # Save checkpoint\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "        }, f'grounding_dino_finetuned_epoch_{epoch+1}.pth')\n",
    "\n",
    "def validate_model(model, processor, val_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            images = batch['images']\n",
    "            text_prompts = batch['text_prompts']\n",
    "            target_boxes = batch['boxes']\n",
    "            \n",
    "            inputs = processor(\n",
    "                images=images,\n",
    "                text=text_prompts,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True\n",
    "            ).to(device)\n",
    "            \n",
    "            target_boxes = [boxes.to(device) for boxes in target_boxes]\n",
    "            \n",
    "            outputs = model(**inputs)\n",
    "            loss = compute_loss(outputs, target_boxes)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    avg_val_loss = total_loss / len(val_loader)\n",
    "    print(f'Validation Loss: {avg_val_loss:.4f}')\n",
    "    return avg_val_loss\n",
    "\n",
    "# Main training script\n",
    "def main():\n",
    "    # Initialize model and processor\n",
    "    model_name = \"IDEA-Research/grounding-dino-base\"\n",
    "    processor = AutoProcessor.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    \n",
    "    # Dataset paths (you'll need to adjust these)\n",
    "    video_dir = \"/path/to/workoutfitness-video/videos\"\n",
    "    annotations_file = \"/path/to/workoutfitness-video/annotations.json\"\n",
    "    \n",
    "    # Create datasets\n",
    "    dataset = WorkoutDataset(video_dir, annotations_file, processor)\n",
    "    \n",
    "    # Split into train/val\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    \n",
    "    # Train model\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    train_grounding_dino(model, processor, train_loader, val_loader, num_epochs=10, device=device)\n",
    "\n",
    "# Inference with fine-tuned model\n",
    "def inference_with_finetuned_model(video_path, output_path, candidate_labels, model_path):\n",
    "    \"\"\"Use fine-tuned model for inference\"\"\"\n",
    "    processor = AutoProcessor.from_pretrained(\"IDEA-Research/grounding-dino-base\")\n",
    "    model = AutoModel.from_pretrained(\"IDEA-Research/grounding-dino-base\")\n",
    "    \n",
    "    # Load fine-tuned weights\n",
    "    checkpoint = torch.load(model_path, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    \n",
    "    # Video processing (similar to previous code)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    output_fps = 8\n",
    "    frame_interval = max(1, fps // output_fps)\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, output_fps, (width, height))\n",
    "    \n",
    "    frame_count = 0\n",
    "    processed_count = 0\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        if frame_count % frame_interval == 0:\n",
    "            pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            text_prompt = \" . \".join(candidate_labels)\n",
    "            \n",
    "            inputs = processor(images=pil_image, text=text_prompt, return_tensors=\"pt\").to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            \n",
    "            # Process outputs and draw detections\n",
    "            # (You'll need to adapt the post-processing based on your training)\n",
    "            \n",
    "            out.write(frame)\n",
    "            processed_count += 1\n",
    "        \n",
    "        frame_count += 1\n",
    "    \n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
